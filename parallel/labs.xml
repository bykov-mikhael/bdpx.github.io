<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="ru">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
<meta name="description" content="Параллельное программирование и распределенные вычисления, MPI: лабораторные работы"/>
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="/styles.css" type="text/css"/>
<title>Параллельное программирование и распределенные вычисления: лабораторные работы</title>
</head>
<body>


<h1>Лабораторные работы</h1>
<p>Для выполнения лабораторных работ используется WMPI-1.3 – полная реализация стандарта Message Passing Interface для платформы Microsoft Win32,
разработанная в университете Coimbra (Португалия).
Это единственная бесплатная реализация, позволяющая реализовать работу коллектива персональных компьютеров начиная с Windows’98.
Библиотека откомпилирована для использования с Microsoft Visual C++ 6 и Digital Visual Fortran 6.0. Необходимые файлы:
</p>

<table>
  <tr>
   <td>wmpi_daemon.exe</td>
   <td>Загрузчик параллельных программ, необходим для удаленного запуска WMPI-приложений</td>
  </tr>
  <tr>
   <td>cwmpi.dll</td>
   <td>Динамическая библиотека функций WMPI.</td>
  </tr>
  <tr>
   <td>bcwmpi.lib, cvwmpi.lib, cdvlibf.lib</td>
   <td>Переходные библиотеки для компоновки с Borland
    C++ (Builder), Microsoft Visual C++ и Visual Fortran соотвественно.
   </td>
  </tr>
  <tr>
   <td>mpi.h</td>
   <td>Заголовочные файлы.</td>
  </tr>
  <tr>
   <td>program.pg</td>
   <td>Файл, описывающий коллектив компьютеров, на которых будет организована работа параллельного приложения.
   </td>
  </tr>
  <tr><td>program.c</td>
   <td>Исходный текст программы
   </td>
  </tr>
 </table>

<p>Для компиляции программ, использующих WMPI, с помощью Borland C++, необходимо: написать консольное приложение (например, program.c), скопировать в рабочую папку файлы «mpi.h» и «bcwmpi.lib», и в командной строке набрать: bcc32 program.c bcwmpi.lib.
</p>
<p>Для компиляции программ, использующих WMPI, с помощью Visual Fortran, необходимо: создать проект консольного приложения,  присоединить к нему библиотеки «cvwmpi.lib» и «cdvlibf.lib», установить: Project Settings  Fortran  Libraries: Multithreaded DLL.
</p>
<p>Для компиляции программ, использующих WMPI, с помощью Visual С++, необходимо создать проект консольного приложения,  присоединить к нему библиотеку «cvwmpi.lib», установить: Project Settings  C  Libraries: Multithreaded DLL.
</p>
<p>Для запуска программ, использующих WMPI, необходимо описать в файле «program.pg» рабочую группу компьютеров, например:
</p>

<pre>local 0 
Lab40-2 1 c:\wmpi\myname\program.exe
Lab40-3 1 c:\wmpi\myname\program.exe
Lab40-4 1 c:\wmpi\myname\program.exe
</pre>

<p>Предполагается, что пользователь работает на машине Lab40-1. На машинах Lab40-2, Lab40-3, Lab40-4 должен быть запущен сервис wmpi_daemon.exe. К папкам «c:\wmpi\myname» на этих машинах должен быть открыт пользователю полный лоступ. В эти папки должны быть помещены копии исходной программы, созданной на машине пользователя L40-1. Можно объединить компиляцию и рассылку программы в одном bat-файле:
</p>

<pre>bcc32 program.c bcwmpi.lib
copy program.exe Lab40-2\\c:\wmpi\myname\program.exe
copy program.exe Lab40-3\\c:\wmpi\myname\program.exe
copy program.exe Lab40-4\\c:\wmpi\myname\program.exe
</pre>

<p>После выполнения подготовительных действий, достаточно на машине пользователя запустить программу «program.exe». В случае «зависания» программы необходимо принудительно завершить работу параллельного приложения на всех машинах коллектива.
</p>

<h2>Задание лабораторных работ</h2>
<ol>
<li>Написание минимального параллельного приложения с помощью «обрамляющих» функций MPI: MPI_Init, MPI_Comm_size, MPI_Comm_rank, MPI_Finalize. Отработка процесса компиляции и запуска параллельного приложения. Как результат, программа должна выводить на печать номер процесса в группе и размер группы.
</li>
<li>Передача сообщений типа точка-точка. Программа должна реализовать передачу данных по кругу, пока когда каждый процесс не получит назад свои данные. Используемые функции: MPI_Send, MPI_Recv, MPI_Sendrecv.
</li>
<li>Коллективные операции. Коллектив машин вычисляет значение определенного одномерного интеграла от какой-либо функции. Машина с номером 0 принимает с консоли исходные данные (отрезок интегрирования и др.), распространяет их среди членов коллектива, затем все вместе вычисляют интеграл (разбив отрезок на равные части), после чего результат собирается на машине с номером 0 (путем коллективного вычисления) и выводится на печать. Используемые функции: MPI_Bcast, MPI_Reduce.
</li>
<li>Коллективные операции. Работа имитирует реальный поиск в параллельной базе данных. Некоторое множество записей (ключ, данные) распределено по машинам коллектива. Пользователь за головной машиной (№0) вводит некоторый ключ.  Головная машина рсапространяет этот ключ в коллективе, после чего все машины ищут у себя записи с таким ключом. Все машины уведомляют головную о количестве найденных ими записей. Головная машина выделяет буфер нужной длины, собирает ото всех машин результаты поиска и выводит на печать. Используемые функции: MPI_Gather, MPI_Gatherv.
</li>
<li>Области связи. Множество процессов делится на две части – четные и нечетные процессы – каждая со своим коммуникатором. Как результат, программа должна выводить на печать номер процесса в новом коммуникаторе и размер новой группы.
</li>
<li>Коллективные операции. Головная машина построчно загружает с консоли квадратную матрицу. Пусть для простоты размерность матрицы кратна числу машин. Головная машина раздает матрицу всем машинам в коллективе (каждой – свое подмножество строк). Реализовать транспонирование уже распределенной матрицы. Используемые функции: MPI_Send, MPI_Recv, MPI_Alltoall, MPI_Alltoallv.
</li>
<li>Пользовательские типы данных. Создать производный тип данных для структуры. Создать производный тип данных для вектора из четных элементов массива. Использовать созданные типы для обмена четных процессов с нечетными соседями.
</li>
<li>Виртуальная топология. Предполагается, что количество процессов в группе равно 4. Головная машина построчно загружает с консоли квадратную матрицу. На множестве процессов вводится декартова топология. Матрица распределяется между процессами в шахматном порядке (например, левый верхний квадрат – процессу (0,0) в декартовой топологии).
</li>
</ol>

</body>
</html>