<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="ru">
<head>
<meta http-equiv="content-type" content="text/html; charset=utf-8"/>
<meta name="description" content="Параллельное программирование и распределенные вычисления, MPI, теория"/>
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="/styles.css" type="text/css"/>

<title>Распределенные вычисления</title>
</head>
<body>

<h1>Распределенные вычисления</h1>
<p>Учебное пособие по курсу “Распределенные вычисления” для студентов математических специальностей.</p>

<p>
Курс лекций “Распределенные вычисления”
предназначен для студентов математических специальностей. В нем излагаются
теретические сведения об архитектуре и классификации современных
высокопроизводительных компьютеров,
рассматриваются стандартные средства
разработки распределенных параллельных
программ на примере библиотеки MPI,
даются практические навыки
по разработке параллельных программ
для вычислительных сетей на основе
персональных компьютеров.</p>
<p>Данное
пособие рекомендовано для проведения
лекционных и практических занятий со
студентами, организации самостоятельной
роботы.</p>

<p>
<b>Содержание</b></p>

<p><br/>
</p>

<ul>
<li class="link"><a href="#__RefHeading___Toc5075823">Параллельные компьютеры и супер-ЭВМ</a></li>
<li class="link"><a href="#__RefHeading___Toc5075824">Источники увеличения производительности ЭВМ</a></li>
<li class="link"><a href="#__RefHeading___Toc5075825">Современная классификация супер-ЭВМ</a></li>
<li class="link"><a href="#__RefHeading___Toc5075826">Факторы, снижающие производительность параллельных компьютеров</a></li>
<li class="link"><a href="#__RefHeading___Toc5075827">Программирование с передачей сообщений</a></li>
<li class="link"><a href="#__RefHeading___Toc5075828">Message Passing Interface</a></li>
<li class="link"><a href="#__RefHeading___Toc5075829">Обрамляющие функции. Начало и завершение.</a></li>
<li class="link"><a href="#__RefHeading___Toc5075830">Простейшие процедуры передачи сообщений типа <q>точка-точка</q>.</a></li>
<li class="link"><a href="#__RefHeading___Toc5075831">Не блокируемые  процедуры передачи сообщений типа <q>точка-точка</q>.</a></li>
<li class="link"><a href="#__RefHeading___Toc5075832">Определение размера сообщения до его помещения в приемный буфер</a></li>
<li class="link"><a href="#__RefHeading___Toc5075833">Процедуры для коллективных коммуникаций</a></li>
<li class="link"><a href="#__RefHeading___Toc5075834">Зачем MPI знать тип передаваемых данных?</a></li>
<li class="link"><a href="#__RefHeading___Toc5075835">Создание и использование собственных типов данных.</a></li>
<li class="link"><a href="#__RefHeading___Toc5075836">Коммуникаторы, группы и области связи.</a></li>
<li class="link"><a href="#__RefHeading___Toc5075837">Виртуальные пользовательские топологии</a></li>
<li class="link"><a href="#__RefHeading___Toc5075838">Второстепенные детали</a></li>
</ul>

<h1 id="__RefHeading___Toc5075823">Параллельные компьютеры и супер-ЭВМ</h1>

<p>К классу супер-ЭВМ принадлежат компьютеры,
которые имеют максимальную производительность
в настоящее время. Быстрое развитие
компьютерной индустрии определяет
относительность данного понятия – то,
что десять лет назад можно было назвать
суперкомпьютером, сегодня под это
определение уже не попадает. Например,
производительность персональных
компьютеров, использующих Pentium-III/800MHz,
сравнима с производительностью
суперкомпьютеров начала 70-х годов,
однако по сегодняшним меркам
суперкомпьютерами не являются ни те,
ни другие. 
</p>

<p>В любом компьютере все основные параметры
тесно связаны. Трудно себе представить
универсальный компьютер, имеющий высокое
быстродействие и мизерную оперативную
память, либо огромную оперативную память
и небольшой объем дисков. Следуя логике,
делаем вывод: супер-ЭВМ это компьютеры,
имеющие в настоящее время не только
максимальную производительность, но и
максимальный объем оперативной и
дисковой памяти. Какие же суперкомпьютеры
существуют в настоящее время в мире?
Вот лишь несколько параметров, дающих
достаточно красноречивую характеристику
машин этого класса. В 2001 году компьютер
ASCI WHITE, занимал первое место в списке
пятисот самых мощных компьютеров мира.
Он объединяет 8192 процессора Power 3 с общей
оперативной памятью в 4 Терабайта и
производительностью более 12 триллионов
операций в секунду. 
</p>
<p>Простые расчеты показывают, что
конфигурации подобных систем могут
стоить не один миллион долларов США -
ради интереса прикиньте, сколько стоят,
скажем, лишь 4 Тбайта оперативной памяти?
Возникает целый ряд естественных
вопросов: какие задачи настолько важны,
что требуются компьютеры стоимостью
несколько миллионов долларов? Или, какие
задачи настолько сложны, что хорошего
Пентиума не достаточно? На эти и подобные
им вопросы хотелось бы найти разумные
ответы. 
</p>
<p>Для
того чтобы оценить сложность решаемых
на практике задач, возьмем конкретную
предметную область, например, оптимизацию
процесса добычи нефти. Имеем подземный
нефтяной резервуар с каким-то числом
пробуренных скважин: по одним на
поверхность откачивается нефть, по
другим обратно закачивается вода. Нужно
смоделировать ситуацию в данном
резервуаре, чтобы оценить запасы нефти
или понять необходимость в дополнительных
скважинах. 
</p>
<p>Примем
упрощенную схему, при которой моделируемая
область отображается в куб, однако и ее
будет достаточно для оценки числа
необходимых арифметических операций.
Разумные размеры куба, при которых можно
получать правдоподобные результаты -
это 100*100*100 точек. В каждой точке куба
надо вычислить от 5 до 20 функций: три
компоненты скорости, давление, температуру,
концентрацию компонент (вода, газ и
нефть - это минимальный набор компонент,
в более реалистичных моделях рассматривают,
например, различные фракции нефти).
Далее, значения функций находятся как
решение нелинейных уравнений, что
требует от 200 до 1000 арифметических
операций. И, наконец, если исследуется
нестационарный процесс, т.е. нужно
понять, как эта система ведет себя во
времени, то делается 100-1000 шагов по
времени. Что получилось: 
</p>
<p>10<sup>6</sup>(точек сетки)*10(функций)*500(операций)*500(шагов по времени) = 2.5*10<sup>12</sup> 
</p>
<p><strong>2500 миллиардов</strong> арифметических операций
для выполнения одного лишь расчета! А
изменение параметров модели? А отслеживание
текущей ситуации при изменении входных
данных? Подобные расчеты необходимо
делать много раз, что накладывает очень
жесткие требования на производительность
используемых вычислительных систем. 
</p>
<p>Примеры
использования суперкомпьютеров можно
найти не только в нефтедобывающей
промышленности. Вот лишь небольшой
список областей человеческой деятельности,
где использование суперкомпьютеров
действительно необходимо: 
</p>
<ul>
 <li>автомобилестроение</li>
 <li>нефте- и газодобыча</li>
 <li>фармакология</li>
 <li>прогноз погоды и моделирование изменения климата</li>
 <li>сейсморазведка</li>
 <li>проектирование электронных устройств</li>
 <li>синтез новых материалов</li>
 <li>и многие, многие другие</li>
</ul>
<p>В 1995 году корпус автомобиля Nissan Maxima удалось
сделать на 10% прочнее благодаря
использованию суперкомпьютера фирмы
Cray (The Atlanta Journal, 28 мая, 1995г). С помощью него
были найдены не только слабые точки
кузова, но и наиболее эффективный способ
их удаления. 
</p>
<p>По данным Марка Миллера (Mark Miller, Ford Motor
Company), для выполнения crash-тестов, при
которых реальные автомобили разбиваются
о бетонную стену с одновременным замером
необходимых параметров, съемкой и
последующей обработкой результатов,
компании Форд понадобилось бы от 10 до
150 прототипов новых моделей при общих
затратах от 4 до 60 миллионов долларов.
Использование суперкомпьютеров позволило
сократить число прототипов на одну
треть. 
</p>
<p>Совсем недавний пример – это развитие одной
из крупнейших мировых систем резервирования
Amadeus, используемой тысячами агенств со
180000 терминалов в более чем ста странах.
Установка двух серверов Hewlett-Packard T600 по
12 процессоров в каждом позволила довести
степень оперативной доступности
центральной системы до 99.85% при текущей
загрузке около 60 миллионов запросов в
сутки. 
</p>
<p>И
подобные примеры можно найти повсюду.
В свое время исследователи фирмы DuPont
искали замену хлорофлюорокарбону. Нужно
было найти материал, имеющий те же
положительные качества: невоспламеняемость,
стойкость к коррозии и низкую токсичность,
но без вредного воздействия на озоновый
слой Земли. За одну неделю были проведены
необходимые расчеты на суперкомпьютере
с общими затратами около 5 тысяч долларов.
По оценкам специалистов DuPont, использование
традиционных экспериментальных методов
исследований потребовало бы около трех
месяцев и 50 тысяч долларов и это без
учета времени, необходимого на синтез
и очистку необходимого количества
вещества. 
</p>
<h1 id="__RefHeading___Toc5075824">Источники увеличения производительности ЭВМ</h1>

<p>А почему суперкомпьютеры считают так быстро?
Вариантов ответа может быть несколько,
среди которых два имеют явное преимущество:
развитие элементной базы и использование
новых решений в архитектуре компьютеров.
</p>

<p>Попробуем разобраться, какой из этих факторов
оказывается решающим для достижения
рекордной производительности. Обратимся
к известным историческим фактам. На
одном из первых компьютеров мира - EDSAC,
появившемся в 1949 году в Кембридже и
имевшем время такта 2 микросекунды
(2*10-6 секунды), можно было выполнить 2*n
арифметических операций за 18*n миллисекунд,
то есть в среднем 100 арифметических
операций в секунду. Сравним с одним
вычислительным узлом современного
суперкомпьютера Hewlett-Packard V2600: время
такта приблизительно 1.8 наносекунды
(1.8*10-9 секунд), а пиковая производительность
около 77 миллиардов арифметических
операций в секунду. 
</p>
<p>Что же получается? За полвека производительность
компьютеров выросла более, чем в <strong>семьсот
миллионов</strong> раз. При этом выигрыш в
быстродействии, связанный с уменьшением
времени такта с 2 микросекунд до 1.8
наносекунд, составляет лишь около 1000
раз. Откуда же взялось остальное? Ответ
очевиден – использование новых решений
в архитектуре компьютеров. Основное
место среди них занимает принцип
параллельной обработки данных, воплощающий
идею одновременного (параллельного)
выполнения нескольких действий. 
</p>
<p>Параллельная обработка данных, воплощая идею
одновременного выполнения нескольких
действий, имеет две разновидности:
конвейерность и собственно параллельность.
Оба вида параллельной обработки
интуитивно понятны, поэтому сделаем
лишь небольшие пояснения. 
</p>
<p><strong>Параллельная обработка</strong>. Если некое устройство
выполняет одну операцию за единицу
времени, то тысячу операций оно выполнит
за тысячу единиц. Если предположить,
что есть пять таких независимых устройств,
способных работать одновременно, то ту
же тысячу операций система из пяти
устройств может выполнить уже не за
тысячу, а за двести единиц времени.
Аналогично система из N устройств ту же
работу выполнит за 1000/N единиц времени.
</p>
<p><strong>Конвейерная обработка</strong>. Что необходимо для
сложения двух вещественных чисел,
представленных в форме с плавающей
запятой? Целое множество мелких операций
таких, как сравнение порядков, выравнивание
порядков, сложение мантисс, нормализация
и т.п. Процессоры первых компьютеров
выполняли все эти <q>микрооперации</q>
для каждой пары аргументов последовательно
одна за другой до тех пор, пока не доходили
до окончательного результата, и лишь
после этого переходили к обработке
следующей пары слагаемых. 
</p>
<p>Идея
конвейерной обработки заключается в
выделении отдельных этапов выполнения
общей операции, причем каждый этап,
выполнив свою работу, передавал бы
результат следующему, одновременно
принимая новую порцию входных данных.
Получаем очевидный выигрыш в скорости
обработки за счет совмещения прежде
разнесенных во времени операций.
Предположим, что в операции можно
выделить пять микроопераций, каждая из
которых выполняется за одну единицу
времени. Если есть одно неделимое
последовательное устройство, то 100 пар
аргументов оно обработает за 500 единиц.
Если каждую микрооперацию выделить в
отдельный этап (или иначе говорят
ступень) конвейерного устройства, то
на пятой единице времени на разной
стадии обработки такого устройства
будут находиться первые пять пар
аргументов, а весь набор из ста пар будет
обработан за 5+99=104 единицы времени –
ускорение по сравнению с последовательным
устройством почти в пять раз (по числу
ступеней конвейера). 
</p>
<p>Казалось
бы, конвейерную обработку можно с успехом
заменить обычным параллелизмом, для
чего продублировать основное устройство
столько раз, сколько ступеней конвейера
предполагается выделить. В самом деле,
пять устройств из предыдущего примера
обработают 100 пар аргументов за 100 единиц
времени, что быстрее времени работы
конвейерного устройства! В чем же дело?
Ответ прост, увеличив в пять раз число
устройств, мы значительно увеличиваем
как объем аппаратуры, так и ее стоимость.
Представьте себе, что на автозаводе
решили убрать конвейер, сохранив темпы
выпуска автомобилей. Если раньше на
конвейере одновременно находилась
тысяча автомобилей, то, действуя по
аналогии с предыдущим примером, надо
набрать тысячу бригад, каждая из которых
в состоянии полностью собрать автомобиль
от начала до конца, выполнив сотни
разного рода операций, и сделать это за
то же время, что машина прежде находилась
на конвейере. Представили себестоимость
такого автомобиля? Нет? Согласен, трудно,
разве что Ламборгини приходит на ум, но
потому и возникла конвейерная обработка...
</p>

<p><em><b>Краткая история появления параллелизма в архитектуре ЭВМ.</b></em>
Сегодня параллелизмом в архитектуре компьютеров
уже мало кого удивишь. Все современные
микропроцессоры, будь то Pentium III или
PA-8700, MIPS R14000, Е2К или Power3 используют тот
или иной вид параллельной обработки. В
ядре Pentium 4 на разных стадиях выполнения
может одновременно находиться до 126
микроопераций. На презентациях новых
чипов и в пресс-релизах корпораций это
преподносится как последнее слово
техники и передовой край науки, и это
действительно так, если рассматривать
реализацию этих принципов в миниатюрных
рамках одного кристалла. 
</p>
<p>Вместе
с тем, сами эти идеи появились очень
давно. Изначально они внедрялись в самых
передовых, а потому единичных, компьютерах
своего времени. Затем после должной
отработки технологии и удешевления
производства они спускались в компьютеры
среднего класса, и, наконец, сегодня все
это в полном объеме воплощается в рабочих
станциях и персональных компьютерах. 
</p>
<p>Для
того чтобы убедиться, что все основные
нововведения в архитектуре современных
процессоров на самом деле используются
еще со времен, когда ни микропроцессоров,
ни понятия суперкомпьютеров еще не
было, совершим маленький экскурс в
историю, начав практически с момента
рождения первых ЭВМ. 
</p>
<p><strong>IBM-701</strong>(1953)
и <strong>IBM-704</strong>(1955): <strong>разрядно
параллельная память, разрядно параллельная
арифметика</strong>. Все самые первые
компьютеры (EDSAC, EDVAC, UNIVAC) имели разрядно
последовательную память, из которой
слова считывались последовательно бит
за битом. Первым коммерчески доступным
компьютером, использующим разрядно
параллельную память и разрядно
параллельную арифметику, стал IBM 701, а
наибольшую популярность получила модель
IBM 704 (продано 150 экз.), в которой, помимо
сказанного, была впервые применена
память на ферритовых сердечниках и
аппаратное АУ с плавающей точкой. 
</p>
<p><strong>IBM-709</strong>(1958):
<strong>независимые процессоры
ввода/вывода</strong>. Процессоры первых
компьютеров сами управляли вводом/выводом.
Однако скорость работы самого быстрого
внешнего устройства, а по тем временам
это магнитная лента, была в 1000 раз меньше
скорости процессора, поэтому во время
операций ввода/вывода процессор
фактически простаивал. В 1958г. к компьютеру
IBM 704 присоединили 6 независимых процессоров
ввода/вывода, которые после получения
команд могли работать параллельно с
основным процессором, а сам компьютер
переименовали в IBM 709. Данная модель
получилась удивительно удачной, так
как вместе с модификациями было продано
около 400 экземпляров, причем последний
был выключен в 1975 году - 20 лет существования!
</p>
<p><strong>IBM-STRETCH</strong>
(1961): <strong>опережающий просмотр
вперед, расслоение памяти</strong>. В
1956 году IBM подписывает контракт с
Лос-Аламосской научной лабораторией
на разработку компьютера STRETCH, имеющего
две принципиально важные особенности:
опережающий просмотр вперед для выборки
команд и расслоение памяти на два банка
для согласования низкой скорости выборки
из памяти и скорости выполнения операций.
</p>
<p><strong>ATLAS</strong>
(1963): <strong>конвейер команд</strong>.
Впервые конвейерный принцип выполнения
команд был использован в машине ATLAS,
разработанной в Манчестерском
университете. Выполнение команд разбито
на 4 стадии: выборка команды, вычисление
адреса операнда, выборка операнда и
выполнение операции. Конвейеризация
позволила уменьшить время выполнения
команд с 6 мкс до 1,6 мкс. Данный компьютер
оказал огромное влияние, как на архитектуру
ЭВМ, так и на программное обеспечение:
в нем впервые использована мультипрограммная
ОС, основанная на использовании
виртуальной памяти и системы прерываний.
</p>
<p><strong>CDC-6600</strong>
(1964): <strong>независимые
функциональные устройства</strong>.
Фирма Control Data Corporation (CDC) при непосредственном
участии одного из ее основателей, Сеймура
Р.Крэя (Seymour R.Cray) выпускает компьютер
CDC-6600 - первый компьютер, в котором
использовалось несколько независимых
функциональных устройств. Для сравнения
с сегодняшним днем приведем некоторые
параметры компьютера: время такта 100нс,
производительность 2-3 млн. операций в
секунду, оперативная память разбита на
32 банка по 4096 60-ти разрядных слов, цикл
памяти 1мкс, 10 независимых функциональных
устройств. Машина имела громадный успех
на научном рынке, активно вытесняя
машины фирмы IBM. 
</p>
<p><strong>CDC
7600</strong> (1969): <strong>конвейерные
независимые функциональные устройства</strong>.
CDC выпускает компьютер CDC-7600 с восемью
независимыми конвейерными функциональными
устройствами - сочетание параллельной
и конвейерной обработки. Основные
параметры: такт 27,5 нс, 10-15 млн. опер/сек.,
8 конвейерных ФУ, 2-х уровневая память. 
</p>
<p><strong>ILLIAC-IV</strong>
(1974): <strong>матричные процессоры</strong>.
Проект: 256 процессорных элементов
(ПЭ) = 4 квадранта по 64ПЭ, возможность
реконфигурации: 2 квадранта по 128ПЭ или
1 квадрант из 256ПЭ, такт 40нс, производительность
1Гфлоп. Работы начаты в 1967 году, к концу
1971 изготовлена система из 1 квадранта,
в 1974г. она введена в эксплуатацию, доводка
велась до 1975 года. Центральная часть:
устройство управления (УУ) + матрица из
64 ПЭ. УУ это простая ЭВМ с небольшой
производительностью, управляющая
матрицей ПЭ; все ПЭ матрицы работали в
синхронном режиме, выполняя в каждый
момент времени одну и ту же команду,
поступившую от УУ, но над своими данными.
ПЭ имел собственное АЛУ с полным набором
команд, ОП - 2Кслова по 64 разряда, цикл
памяти 350нс, каждый ПЭ имел непосредственный
доступ только к своей ОП. Сеть пересылки
данных: двумерный тор со сдвигом на 1 по
границе по горизонтали. 
</p>
<p>Несмотря
на результат в сравнении с проектом:
стоимость в 4 раза выше, сделан лишь 1
квадрант, такт 80нс, реальная произв-ть
до 50Мфлоп – данный проект оказал огромное
влияние на архитектуру последующих
машин, построенных по схожему принципу,
в частности: PEPE, BSP, ICL DAP.</p>
<p><strong>CRAY-1</strong>
(1976): <strong>векторно-конвейерные процессоры. </strong>В 1972 году С.Крэй
покидает CDC и основывает свою компанию
Cray Research, которая в 1976г. выпускает первый
векторно-конвейерный компьютер CRAY-1:
время такта 12.5нс, 12 конвейерных
функциональных устройств, пиковая
производительность 160 миллионов операций
в секунду, оперативная память до 1Мслова
(слово - 64 разряда), цикл памяти 50нс.
Главным новшеством является введение
векторных команд, работающих с целыми
массивами независимых данных и позволяющих
эффективно использовать конвейерные
функциональные устройства. 
</p>
<p><strong>Иерархия памяти. </strong>Иерархия памяти прямого
отношения к параллелизму не имеет,
однако, безусловно, относится к тем
особенностям архитектуры компьютеров,
которые имеет огромное значение для
повышения их производительности
(сглаживание разницы между скоростью
работы процессора и временем выборки
из памяти). Основные уровни: регистры,
кэш-память, оперативная память, дисковая
память. Время выборки по уровням памяти
от дисковой памяти к регистрам уменьшается,
стоимость в пересчете на 1 слово (байт)
растет. В настоящее время, подобная
иерархия поддерживается даже на
персональных компьютерах. 
</p>
<h1 id="__RefHeading___Toc5075825">Современная классификация супер-ЭВМ</h1>
<p>По каким
же направлениям идет развитие
высокопроизводительной вычислительной
техники в настоящее время? Основных
направлений пять: <b>PVP, SMP, NUMA, MPP, кластеры.</b>
</p>
<p>Основным
параметром классификации параллельных
компьютеров является наличие общей
(SMP) или распределенной памяти (MPP). Нечто
среднее между SMP и MPP представляют собой
NUMA-архитектуры, где память физически
распределена, но логически общедоступна.
Кластерные системы являются более
дешевым вариантом MPP. При поддержке
команд обработки векторных данных
говорят о векторно-конвейерных
процессорах, которые, в свою очередь
могут объединяться в PVP-системы с
использованием общей или распределенной
памяти. Все большую популярность
приобретают идеи комбинирования
различных архитектур в одной системе
и построения неоднородных систем. При
организациях распределенных вычислений
в глобальных сетях (Интернет) говорят
о мета-компьютерах, которые, строго
говоря, не представляют собой параллельных
архитектур. 
</p>
<p><em><b>Параллельные
векторные системы (PVP).</b></em> Конвейерные
функциональные устройства и набор
векторных команд – это две особенности
таких машин. В отличие от традиционного
подхода, векторные команды оперируют
целыми массивами независимых данных,
что позволяет эффективно загружать
доступные конвейеры, т.е. команда вида
A=B+C может означать сложение двух массивов,
а не двух чисел. Характерным представителем
данного направления является семейство
векторно-конвейерных компьютеров CRAY,
куда входят, например, CRAY EL, CRAY J90, CRAY T90.
</p>
<p><b>Архитектура.</b>
Основным признаком PVP-систем является
наличие специальных векторно-конвейерных
процессоров, в которых предусмотрены
команды однотипной обработки векторов
независимых данных, эффективно
выполняющиеся на конвейерных функциональных
устройствах. Как правило, несколько
таких процессоров (1-16) работают
одновременно над общей памятью (аналогично
SMP) в рамках многопроцессорных конфигураций.
Несколько таких узлов могут быть
объединены с помощью коммутатора
(аналогично MPP).</p>
<p><b>Примеры.
</b>NEC SX-4/SX-5, линия векторно-конвейерных
компьютеров CRAY: от CRAY-1, CRAY J90/T90, CRAY SV1,
серия Fujitsu VPP.</p>
<p><b>Модель
программирования. </b>Эффективное
программирование подразумевает
векторизацию циклов (для достижения
разумной производительности одного
процессора) и их распараллеливание (для
одновременной загрузки нескольких
процессоров одним приложением).</p>
<p><em><b>Симметричные
мультипроцессорные системы (SMP).</b></em>
Параллельные компьютеры с общей памятью.
Вся оперативная память таких компьютеров
разделяется несколькими одинаковыми
процессорами. Это снимает проблемы
предыдущего класса, но добавляет новые
- число процессоров, имеющих доступ к
общей памяти, по чисто техническим
причинам нельзя сделать большим. В
данное направление входят многие
современные многопроцессорные
SMP-компьютеры или, например, отдельные
узлы компьютеров HP Exemplar и Sun
StarFire. 
</p>
<p><b>Архитектура.
</b>Система состоит из нескольких
однородных процессоров и массива общей
памяти (обычно из нескольких независимых
блоков). Все процессоры имеют доступ к
любой точке памяти с одинаковой скоростью.
Процессоры подключены к памяти либо с
помощью общей шины (базовые 2-4 процессорные
SMP-сервера), либо с помощью crossbar-коммутатора
(HP 9000). Аппаратно поддерживается
когерентность кэшей. 
</p>
<p><b>Примеры.
</b>HP 9000 V-class, N-class; SMP-cервера и рабочие
станции на базе процессоров Intel (IBM, HP,
Compaq, Dell, ALR, Unisys, DG, Fujitsu и др.). 
</p>
<p><b>Масштабируемость.
</b>Наличие общей памяти сильно упрощает
взаимодействие процессоров между собой,
однако накладывает сильные ограничения
на их число - не более 32 в реальных
системах. Для построения масштабируемых
систем на базе SMP используются кластерные
или NUMA-архитектуры. 
</p>
<p><b>Операционная
система. </b>Вся система работает под
управлением единой ОС (обычно UNIX-подобной,
но для Intel-платформ поддерживается
Windows NT). ОС автоматически (в процессе
работы) распределяет процессы/нити по
процессорам (scheduling), но иногда возможна
и явная привязка. 
</p>
<p><b>Модель
программирования. </b>Программирование
в модели <dfn>общей памяти</dfn>. (POSIX
threads, OpenMP, другие средства поддержки
многопоточности). Для SMP-систем существуют
сравнительно эффективные средства
автоматического распараллеливания. 
</p>
<p><em><b>Системы
с неоднородным доступом к памяти (NUMA).</b></em>
Система состоит из однородных базовых
модулей (плат), состоящих из небольшого
числа процессоров и блока памяти. Модули
объединены с помощью высокоскоростного
коммутатора. Поддерживается единое
адресное пространство, аппаратно
поддерживается доступ к удаленной
памяти, т.е. к памяти других модулей. При
этом доступ к локальной памяти в несколько
раз быстрее, чем к удаленной памяти. В
случае если аппаратно поддерживается
когерентность кэшей во всей системе
(обычно это так), говорят об архитектуре
<dfn>cc-NUMA</dfn> (cache-coherent NUMA). Модель
программирования аналогична SMP. 
</p>
<p><b>Примеры.
</b>HP HP 9000 V-class в SCA-конфигурациях, SGI
Origin2000, Sun HPC 10000, IBM/Sequent NUMA-Q 2000, SNI RM600. 
</p>
<p><b>Масштабируемость
</b>NUMA-систем ограничивается объемом
адресного пространства, возможностями
аппаратуры поддежки когерентности
кэшей и возможностями операционной
системы по управлению большим числом
процессоров. На настоящий момент,
максимальное число процессоров в
NUMA-системах составляет 256 (Origin2000). 
</p>
<p><b>Операционная
система. </b>Обычно вся система работает
под управлением единой ОС, как в SMP. Но
возможны также варианты динамического
<q>подразделения</q> системы, когда
отдельные <q>разделы</q> системы работают
под управлением разных ОС (например,
Windows NT и UNIX в NUMA-Q 2000). 
</p>

<p><em><b>Массивно-параллельные системы (MPP).</b></em>
Параллельные компьютеры с распределенной
памятью. Идея построения компьютеров
этого класса тривиальна: возьмем серийные
микропроцессоры, снабдим каждый своей
локальной памятью, соединим посредством
некоторой коммуникационной среды –
вот и все. Достоинств у такой архитектуры
масса: если нужна высокая производительность,
то можно добавить еще процессоров, если
ограничены финансы или заранее известна
требуемая вычислительная мощность, то
легко подобрать оптимальную конфигурацию
и т.п. Однако есть и решающий <q>минус</q>,
сводящий многие <q>плюсы</q> на нет.
Дело в том, что межпроцессорное
взаимодействие в компьютерах этого
класса идет намного медленнее, чем
происходит локальная обработка данных
самими процессорами. Именно поэтому
написать эффективную программу для
таких компьютеров очень сложно, а для
некоторых алгоритмов иногда просто
невозможно. К данному классу можно
отнести компьютеры Intel Paragon, IBM SP1, Parsytec,
в какой-то степени IBM SP2 и CRAY T3D/T3E, хотя в
этих компьютерах влияние указанного
минуса значительно ослаблено. К этому
же классу можно отнести и сети компьютеров,
которые все чаще рассматривают как
дешевую альтернативу крайне дорогим суперкомпьютерам.
</p>

<p>Распределенность памяти означает то,
что каждый процессор имеет непосредственный
доступ только к своей локальной памяти,
а доступ к данным, расположенным в памяти
других процессоров, выполняется другими,
более сложными способами. 
</p>

<p>Основные причины появления массивно-параллельных
компьютеров - это, во-первых, необходимость
построения компьютеров с гигантской
производительностью, и, во-вторых,
необходимость производства компьютеров
в большом диапазоне, как производительности,
так и стоимости. Не все в состоянии
купить однопроцессорный CRAY Y-MP C90, да и
не всегда такие мощности нужны. Для
массивно-параллельного компьютера, в
котором число процессоров может сильно
меняться, всегда можно подобрать
конфигурацию с заранее заданной
производительностью и/или стоимостью.
</p>
<p><b>Архитектура.
</b>Система состоит из однородных
<em>вычислительных узлов</em>, включающих:
один или несколько центральных
процессоров, локальную память (прямой
доступ к памяти других узлов невозможен),
коммуникационный процессор или сетевой
адаптер, иногда - жесткие диски (как в
SP) и/или другие устройства ввода/вывода.
К системе могут быть добавлены специальные
узлы ввода-вывода и управляющие узлы. 
</p>
<p><em>Сетевой
интерфейс</em> формирует передачи перед
посылкой через коммуникационную сеть
другим вычислительным узлам или узлам
ввода/вывода, а также принимает приходящие
сообщения (без прерывания работы
микропроцессора). 
</p>
<p>Узлы
связаны через некоторую коммуникационную
среду, например: Intel Paragon – двумерная
прямоугольная решетка, IBM SP2 – коммутатор,
CRAY T3D – трехмерный тор, nCube-10
– гиперкуб, а также разного рода
деревья.</p>
<p>Коммуникационная
сеть организуется таким образом, чтобы:
минимизировать среднее число перемещений
пакетов по сети при взаимодействии
разных узлов; дать возможность выбора
другого маршрута для обхода поврежденных
связей; максимизировать пропускную
способность сети, удалить узкие места;
минимизировать длину максимального
физического соединения между узлами.</p>
<p><b>Примеры.
</b>IBM RS/6000 SP2, Intel PARAGON/ASCI Red, CRAY T3E, Hitachi
SR8000, транспьютерные системы Parsytec. 
</p>
<p><b>Масштабируемость.
</b>Общее число процессоров в реальных
системах достигает нескольких тысяч
(ASCI Red, Blue Mountain). 
</p>
<p><b>Операционная
система. </b>Существуют два основных
варианта. Либо полноценная ОС работает
только на управляющей машине (front-end), на
каждом узле работает сильно урезанный
вариант ОС, обеспечивающие только работу
расположенной в нем ветви параллельного
приложения. Пример: Cray T3E. Либо на каждом
узле работает полноценная UNIX-подобная
ОС (вариант, близкий к кластерному
подходу). Пример: IBM RS/6000 SP + ОС AIX,
устанавливаемая отдельно на каждом
узле. 
</p>
<p><b>Модель
программирования. </b>Программирование
в рамках модели <dfn>передачи сообщений</dfn>
(коммуникационные библиотеки MPI, PVM,
BSPlib). 
</p>
<p><em><b>Кластерные
системы.</b></em> Последнее направление,
строго говоря, не является самостоятельным,
а скорее представляет собой комбинации
предыдущих трех. Из нескольких процессоров
(традиционных или векторно-конвейерных)
и общей для них памяти сформируем
вычислительный узел. Если полученной
вычислительной мощности не достаточно,
то объединим несколько узлов
высокоскоростными каналами. Подобную
архитектуру называют кластерной, и по
такому принципу построены CRAY SV1, HP
Exemplar, Sun StarFire, NEC SX-5, последние модели IBM
SP2 и другие. Именно это направление
является в настоящее время наиболее
перспективным для конструирования
компьютеров с рекордными показателями
производительности. 
</p>
<p><b>Архитектура.
</b>Набор рабочих станций (или даже ПК)
общего назначения, используется в
качестве дешевого варианта
массивно-параллельного компьютера. Для
связи узлов используется одна из
стандартных сетевых технологий
(Fast/Gigabit Ethernet, Myrinet) на базе шинной
архитектуры или коммутатора. При
объединении в кластер компьютеров
разной мощности или разной архитектуры,
говорят о <dfn>гетерогенных</dfn>
(неоднородных) кластерах. Узлы кластера
могут одновременно использоваться в
качестве пользовательских рабочих
станций. В случае, когда это не нужно,
узлы могут быть существенно облегчены
и/или установлены в стойку. 
</p>
<p><b>Примеры.
</b>NT-кластер в NCSA, Beowulf-кластеры. 
</p>
<p><b>Операционная
система. </b>Используются стандартные
для рабочих станций ОС, чаще всего,
свободно распространяемые - Linux/FreeBSD,
вместе со специальными средствами
поддержки параллельного программирования
и распределения нагрузки. 
</p>
<p><b>Модель
программирования. </b>Программирование,
как правило, в рамках модели передачи
сообщений (чаще всего – MPI). Дешевизна
подобных систем оборачивается большими
накладными расходами на взаимодействие
параллельных процессов между собой,
что сильно сужает потенциальный класс
решаемых задач. 
</p>

<h1 id="__RefHeading___Toc5075826">Факторы, снижающие производительность параллельных компьютеров</h1>

<p>К сожалению, чудеса в жизни редко случаются.
Гигантская производительность
параллельных компьютеров и супер-ЭВМ
с лихвой компенсируется сложностями
их использования. Начнем с самых простых
вещей. У вас есть программа и доступ,
скажем, к 256-процессорному компьютеру.
Что вы ожидаете? Да ясно что: вы вполне
законно ожидаете, что программа будет
выполняться в 256 раз быстрее, чем на
одном процессоре. А вот этого, скорее
всего, и не будет. 
</p>
<p>Для
достижения эффективной параллельной
обработки необходимо добиться <strong>равномерной
загрузки всех процессоров</strong>. Если
равномерности нет, то часть процессоров
неизбежно будет простаивать, ожидая
остальных, хотя в этот момент они могли
бы выполнять полезную работу. Иногда
равномерность получается автоматически,
например, при обработке прямоугольных
матриц достаточно большого размера,
однако уже при переходе к треугольным
матрицам добиться хорошей равномерности
не так просто. 
</p>
<p>Если
это компьютер с распределенной памятью,
то взаимодействие процессоров, в
основном, осуществляется посредством
передачи сообщений друг другу. Отсюда
два других замедляющих фактора – <b>время
инициализации посылки сообщения</b>
(латентность) и собственно <b>время
передачи сообщения по сети</b>. Максимальная
скорость передачи достигается на больших
сообщениях, когда латентность, возникающая
лишь в начале, не столь заметна на фоне
непосредственно передачи данных.</p>
<p><b>Возможность
асинхронной посылки сообщений и
вычислений</b>. Если или аппаратура, или
программное обеспечение не поддерживают
возможности проводить вычислений на
фоне пересылок, то возникнут неизбежные
накладные расходы, связанные с ожиданием
полного завершения взаимодействия
параллельных процессов. 
</p>
<p>Если
один процессор должен вычислить некоторые
данные, которые нужны другому процессору,
и если второй процесс первым дойдет до
точки приема соответствующего сообщения,
то он с неизбежностью будет простаивать,
ожидая передачи. Для того чтобы
минимизировать <b>время ожидание прихода
сообщения</b> первый процесс должен
отправить требуемые данные как можно
раньше, отложив независящую от них
работу на потом, а второй процесс должен
выполнить максимум работы, не требующей
ожидаемой передачи, прежде, чем выходить
на точку приема сообщения. 
</p>
<p>Чтобы
не сложилось совсем плохого впечатления
о массивно-параллельных компьютерах,
надо заканчивать с негативными факторами,
потому последний фактор - это <b>реальная
производительность одного процессора</b>.
Разные модели микропроцессоров могут
поддерживать несколько уровней
кэш-памяти, иметь специализированные
функциональные устройства, регистровую
структуру и т.п. Каждый микропроцессор,
в конце концов, может иметь
векторно-конвейерную архитектуру. 
</p>
<p>К
сожалению, на работе каждой конкретной
программы сказываются в той или иной
мере все эти факторы одновременно,
дополнительно усугубляя ситуацию с
эффективностью параллельных программ.
Однако в отличие от векторно-конвейерных
компьютеров все изложенные здесь
факторы, за исключением, быть может,
последнего, могут снизить производительность
не в десятки, а в сотни и даже тысячи раз
по сравнению с пиковыми показателями
производительности компьютера. Добиться
на этих компьютерах, в принципе, можно
многого, но усилий это может потребовать
во многих случаях очень больших. 
</p>

<p><b>Закон Амдала.</b> Предположим, что в
вашей программе доля операций, которые
нужно выполнять последовательно, равна
f, где 0&lt;=f&lt;=1 (при этом доля понимается
не по статическому числу строк кода, а
по числу операций в процессе выполнения).
Крайние случаи в значениях f соответствуют
полностью параллельным (f=0) и полностью
последовательным (f=1) программам. Так
вот, для того, чтобы оценить, какое
ускорение S может быть получено на
компьютере из 'p' процессоров при данном
значении f, можно воспользоваться законом Амдала:
</p>

<p><b>S &lt;= 1 / (f + (1-f) / p)</b></p>

<p>Если 9/10 программы исполняется параллельно,
а 1/10 по-прежнему последовательно, то
ускорения более, чем в 10 раз получить в
принципе невозможно вне зависимости
от качества реализации параллельной
части кода и числа используемых
процессоров (ясно, что 10 получается
только в том случае, когда время исполнения
параллельной части равно 0). 
</p>
<p>Ниже в
таблице показано, на какое максимальное
ускорение работы программы можно
рассчитывать в зависимости от доли
последовательных вычислений и числа
доступных процессоров. Предполагается,
что параллельная секция может быть
выполнена без каких-либо дополнительных
накладных расходов. Так как в программе
всегда присутствует инициализация,
ввод/вывод и некоторые сугубо
последовательные действия, то недооценивать
данный фактор никак нельзя – практически
вся программа должна исполняться в
параллельном режиме, что можно обеспечить
только после анализа всей (!) программы.
</p>
<p><br/>
</p>

<table>
<caption>Максимальное ускорение работы программы в зависимости от доли последовательных вычислений и числа используемых процессоров.</caption>
<thead>
<tr>
<th rowspan="2">Число процессоров</th>
<th colspan="5">Доля последовательных вычислений</th>
</tr>
<tr>
<th>50%</th>
<th>25%</th>
<th>10%</th>
<th>5%</th>
<th>2%</th>
</tr>
</thead>
<tbody>
  <tr>
   <td>2</td>
   <td>1.33</td>
   <td>1.60</td>
   <td>1.82</td>
   <td>.90</td>
   <td>1.96</td>
  </tr>
  <tr>
   <td>8</td>
   <td>1.78</td>
   <td>2.91</td>
   <td>4.71</td>
   <td>5.93</td>
   <td>7.02</td>
  </tr>
  <tr>
   <td>32</td>
   <td>1.94</td>
   <td>3.66</td>
   <td>7.80</td>
   <td>12.55</td>
   <td>19.75</td>
  </tr>
  <tr>
   <td>512</td>
   <td>1.99</td>
   <td>3.97</td>
   <td>9.83</td>
   <td>19.28</td>
   <td>45.63</td>
  </tr>
  <tr>
   <td>2048</td>
   <td>2.00</td>
   <td>3.99</td>
   <td>9.96</td>
   <td>19.82</td>
   <td>48.83</td>
  </tr>
</tbody>
</table>

<p>Посмотрим на проблему с другой стороны. А какую
же часть кода надо ускорить (а значит и
предварительно исследовать), чтобы
получить заданное ускорение? Ответ
можно найти в следствии из закона Амдала:
для того чтобы ускорить выполнение
программы в <em>q</em> раз необходимо ускорить
не менее, чем в <i>q</i> раз не менее, чем
(1-1/<em>q</em>)-ю часть программы. Следовательно,
если есть желание ускорить программу
в 100 раз по сравнению с ее последовательным
вариантом, то необходимо получить не
меньшее ускорение не менее, чем на 99.99%
кода, что почти всегда составляет
значительную часть программы! 
</p>
<p>Отсюда
первый вывод – прежде, чем основательно
переделывать код для перехода на
параллельный компьютер (а любой
суперкомпьютер, в частности, является
таковым) надо основательно подумать.
Если, оценив заложенный в программе
алгоритм, вы поняли, что доля последовательных
операций велика, то на значительное
ускорение рассчитывать явно не приходится
и нужно думать о замене отдельных
компонент алгоритма. 
</p>
<p>В ряде
случаев последовательный характер
алгоритма изменить не так сложно.
Допустим, что в программе есть следующий
фрагмент для вычисления суммы n чисел:
</p>
<p>    s
= 0</p>
<p>    Do
i = 1, n</p>
<p>       s
= s + a(i)</p>
<p>    EndDo</p>
<p>По своей
природе он строго последователен, так
как на i-й итерации цикла требуется
результат с (i-1)-й и все итерации выполняются
одна за другой. Имеем 100% последовательных
операций, а значит и никакого эффекта
от использования параллельных компьютеров.
Вместе с тем, выход очевиден. Поскольку
в большинстве реальных программ (вопрос:
а почему в большинстве, а не во всех?)
нет существенной разницы, в каком порядке
складывать числа, выберем иную схему
сложения. Сначала найдем сумму пар
соседних элементов: a(1)+a(2), a(3)+a(4), a(5)+a(6)
и т.д. Заметим, что при такой схеме все
пары можно складывать одновременно! На
следующих шагах будем действовать
абсолютно аналогично, получив вариант
параллельного алгоритма. 
</p>
<p>Казалось
бы, в данном случае все проблемы удалось
разрешить. Но представьте, что доступные
вам процессоры разнородны по своей
производительности. Значит, будет такой
момент, когда кто-то из них еще трудится,
а кто-то уже все сделал и бесполезно
простаивает в ожидании. Если разброс в
производительности компьютеров большой,
то и эффективность всей системы при
равномерной загрузке процессоров будет
крайне низкой. 
</p>
<p>Но
пойдем дальше и предположим, что все
процессоры одинаковы. Проблемы кончились?
Опять нет! Процессоры выполнили свою
работу, но результат-то надо передать
дальше для продолжения процесса
суммирования... а на передачу уходит
время... и в это время процессоры опять
простаивают... 
</p>
<p>Заставить
параллельную вычислительную систему
или супер-ЭВМ работать с максимальной
эффективностью на конкретной программе
– задача не из простых, поскольку
необходимо
тщательное согласование структуры
программ и алгоритмов с особенностями
архитектуры параллельных вычислительных
систем. 
</p>
<p>Как вы
думаете, верно ли утверждение: чем мощнее
компьютер, тем быстрее на нем можно
решить данную задачу? Нет, это не верно.
Это можно пояснить простым бытовым
примером. Если один землекоп выкопает
яму в 1м<sup>3</sup> за 1 час, то два таких же
землекопа это сделают за 30 мин - в это
можно поверить. А за сколько времени
эту работу сделают 60 землекопов? За 1
минуту? Конечно же, нет! Начиная с
некоторого момента, они будут просто
мешать друг другу, не ускоряя, а замедляя
процесс. Так же и в компьютерах: если
задача слишком мала, то мы будем дольше
заниматься распределением работы,
синхронизацией процессов, сборкой
результатов и т.п., чем непосредственно
полезной работой. 
</p>

<h1 id="__RefHeading___Toc5075827">Программирование с передачей сообщений</h1>

<p><b>Распределенная память.</b> Каждый процессор имеет свою собственную локальную память,
к которой может непосредственно обращаться только этот процессор. Передача данных от одного процессора другому осуществляется
посредством сети, в отличие от  систем с разделяемой памятью, в которых несколько процессоров имеют прямой доступ к общей
памяти через общую шину или коммутатор.
</p>

<p><b>Передача сообщений.</b>
Метод копирования данных из памяти
одного процессора в память другого
процессора. В системах с распределенной
памятью, данные обычно посылаются как
пакеты информации через межпроцессорную
сеть. Сообщение может состоять из одного
или более пакетов, и обычно включает
дополнительную контрольную информацию.
</p>

<p><b>Процесс</b>. Процесс – набор
выполнимых инструкций (программа),
которая выполняется на процессоре.
Один или большее количество
процессов может выполнять на процессоре.
В системе с передачей
сообщений, все процессы связываются
друг с другом,  посылая сообщения –
даже если они выполняются на
том же самом процессоре. Однако
по причинам эффективности системы с
передачей сообщений обычно назначают
только один процесс на процессор. 
</p>

<p><b>Библиотека передачи сообщений.</b>
Собрание подпрограмм, которые используются
в программном коде, чтобы выполнить
посылку, прием и другие действия по
передаче сообщений. 
</p>

<p><b>Посылка/Прием.</b> Передача сообщений
вызывает перемещение данных от одного
процесса (посылка) другому (прием).
Требуется сотрудничество обоих процессов
– получателя и отправителя. Операции
посылки обычно требуют, чтобы  процесс
посылки определил местоположение
данных, размер, тип и процесс назначения.
Операция приема, должна совпадать с
соответствующей операцией посылки на
другой стороне. 
</p>

<p>
<b>Синхронный/асинхронный обмен.</b>
Синхронные операции посылки,  заканчиваются только после подтверждения, что сообщение было благополучно получено приемником
получения. Асинхронные операции посылки, могут закончиться, даже если процесс получения фактически не получил сообщение.
</p>

<p><b>Прикладной буфер</b>. Адресное пространство, где хранятся данные, которые должны быть посланы или получены.
</p>

<p>
<b>Системный буфер.</b> Системное адресное пространство для хранения сообщений.
В зависимости от типа операции посылки (приема), данные в прикладном буфере могут быть скопированы в системный буфер (из системного буфера).
Это позволяет коммуникационным связям быть асинхронными.
</p>

<p>
<b>Блокируемые связи.</b> Коммуникационные
подпрограммы блокируемы, если завершение
запроса зависит от некоторых <q>событий</q>.
Для посылок, данные должны быть успешно
посланы или благополучно скопированы
в системный буфер так, что прикладной
буфер, который содержал данные, далее
доступен для повторного использования.
Для приемов, данные должны быть
благополучно записаны в приемном буфере
так, что буфер готов к использованию. 
</p>
<p>
<b>Не блокируемые связи.</b> Коммуникационные
подпрограммы не блокируемы, если запрос
на передачу данных возвращает управление,
не ожидая завершения коммуникаций (как
копирование сообщения из памяти
пользователя в системную память или
прибытие сообщения). Небезопасно изменять
или использовать прикладной буфер сразу
после завершения не блокируемой передачи.
На программисте лежит ответственность
страховать, что прикладной буфер свободен
для повторного использования. Не
блокируемые коммуникации, прежде всего,
используются для наложения во времени
вычислений и передачи данных, что
позволяет увеличить производительность
работы. 
</p>

<h1 id="__RefHeading___Toc5075828">Message Passing Interface</h1>

<p>MPI – это программный инструментарий
для обеспечения связи между ветвями
параллельного приложения. MPI расшифровывается
как <q>Message passing interface</q> (<q>Взаимодействие через передачу сообщений</q>). Несколько
путает дело тот факт, что этот термин
уже применяется по отношению к аппаратной
архитектуре ЭВМ. Программный инструментарий
MPI реализован, в том числе и для ЭВМ с
такой архитектурой. MPI предоставляет
программисту единый механизм взаимодействия
ветвей внутри параллельного приложения.
MPI не зависит от машинной архитектуры:
однопроцессорные или многопроцессорные
системы с общей или раздельной памятью.
MPI не зависит от взаимного расположения
ветвей – на одном процессоре или на
разных. MPI не зависит от API операционной
системы (API = <q>applications programmers interface</q> = <q>интерфейс разработчика приложений</q>).
Весь параллелизм явен: программист
ответствен за правильное выделение
параллелизма, и осуществление алгоритма,
используя конструкции MPI. 
</p>
<p>
<i><em><b>Message Passing Interface</b></em></i>:
спецификация библиотеки передачи
сообщений, разработанная
в качестве стандарта для
параллельных вычислительных систем
с распределенной памятью.
Цель разработки MPI
состоит в том, чтобы обеспечить широко
используемый стандарт для написания
программ в парадигме обмена сообщениями.
Интерфейс устанавливает переносимый,
эффективный, и гибкий стандарт для
передачи сообщений. 
</p>
<p>
<i><em><b>История</b></em></i><i><em><b>
появления </b></em></i><i><em><b>MPI: </b></em></i>
</p>
<p>
1980-е - начало
1990-х: Развиваются
системы с распределенной
памятью и
параллельные
вычисления,
появляется
множество несовместимых инструментов
программного обеспечения для написания
таких программ –
обычно в виде
компромисса между
мобильностью, производительностью,
функциональными возможностями и ценой.
Возникает
потребность
в едином стандарте.

</p>
<p>
Апрель 1992: Симпозиум по
стандартам для сообщений
в среде с распределенной
памятью,
Вильямсбург Вирджиния.
Обсуждены основные
особенности, существенные для стандарта
MPI, организована рабочая
группа для продолжения
процесса стандартизации.
Принят предварительный
проект стандарта. 
</p>
<p>
Ноябрь 1992: Рабочая группа
встречается в Миннеаполисе.
Представлен проект MPI
(MPI-1).
В целом принята структура MPI.
Для доработки организован
MPI Форум – приблизительно
175 человек из
40 организаций, включая производителей
параллельных компьютеров,
авторов программного обеспечения,
академических и прикладных
ученых. 
</p>
<p>
Ноябрь 1993: Конференция
Supercomputing 93 – представлен
проект стандарта MPI. 
</p>
<p>
Май 1994:
Выпущена заключительная
версия спецификации проекта.

</p>
<p>
<i><em><b>Причины для использования
MPI:</b></em></i> MPI
– единственная библиотека
передачи сообщений,
которая может рассматриваться как
стандарт. Она
поддерживается фактически
на всех высокопроизводительных
компьютерах и локальных сетях.
Мобильность – нет необходимости
изменять исходный код при переносе
приложения на другую платформу, которая
поддерживает MPI. Производительность –
реализация MPI от производителя
должна быть
способна использовать
родные особенности аппаратных
средств ЭВМ, чтобы оптимизировать
производительность.
Функциональные возможности – более
чем 115 процедур. Доступность – доступны
разнообразные коммерческие и
открытые реализации. 
</p>
<p>
Несколько вычислительных устройств
(процессоров), по нынешней терминологии,
могут быть объединены одним из трех
способов:
</p>

<ol>
<li>через общую память;</li>
<li>через скоростную внутримашинную сеть на ЭВМ с аппаратной архитектурой MPI;</li>
<li>через локальную сеть, работающую, как правило, по протоколу TCP/IP.</li>
</ol>

<p>Целевая платформа MPI – системы с распределенной
памятью, включая массивно-параллельные
машины, SMP кластеры, группы рабочих
станций и гетерогенные сети из персональных
компьютеров. Программный инструментарий
MPI разрабатывался как долгожданный
стандарт для машин с одноименной
аппаратной архитектурой. Задачей-максимумом
было <q>подмять</q> под него и стандарты
из соседних пунктов: MPI может работать
на базе любого из трех способов соединений.
</p>
<p>Первая
редакция MPI стандартом не стала в силу
следующего ограничения: число задач в
параллельной программе статично, все
процессы, сообщающиеся между собой
посредством функций MPI, начинают и
заканчивают свое выполнение <i><em>одновременно</em></i>.
Это не мешает использовать MPI как скелет
для параллельных приложений, но системы
массового обслуживания (клиент-серверные
приложения и проч.) приходится разрабатывать
на базе старого инструментария. Проблема
устранена в MPI-2: новые задачи могут быть
динамически порождены во время исполнения.
MPI-2 гарантировано станет стандартом в
категории 2. Не исключено, что и стандарты
категорий 1 и 3 со временем потеряют
интерес для программиста, и будут
использоваться только как интерфейс
между <q>железом</q> и MPI-2.
</p>
<p>Минимально
в состав MPI входят: библиотека
программирования (заголовочные и
библиотечные файлы для языков Си, Си++
и Фортран) и загрузчик приложений.
Дополнительно включаются: профилирующий
вариант библиотеки (используется на
стадии тестирования параллельного
приложения для определения оптимальности
распараллеливания); загрузчик с
графическим и сетевым интерфейсом для
X-Windows и проч. Минимальный набор функций
прост в освоении и позволяет быстро
написать надежно работающую программу.
Использование же всей мощи MPI позволит
получить <i><em>быстро</em></i> работающую
программу – при сохранении надежности.
</p>
<p><i><em><b>Сравнение
с низкоуровневыми пересылками.</b></em></i>
Часто приходится слышать утверждение,
что низкоуровневые пересылки через
разделяемую память и семафоры (для
систем с общей памятью) или использование
конкретного транспортного протокола
(для систем с распределенной памятью)
предпочтительнее применения таких
библиотек как MPI, потому что работает
быстрее. Против такой точки зрения
существуют веские доводы: 
</p>
<p>В хорошо
распараллеленном приложении на собственно
взаимодействие между ветвями (пересылки
данных и синхронизацию) тратится
небольшая доля времени – несколько
процентов от общего времени работы.
Таким образом, замедление пересылок,
допустим в два раза, не означает общего
падения производительности вдвое –
она понизится лишь на несколько процентов.
Зачастую такое понижение производительности
является приемлемым и с лихвой
оправдывается прочими соображениями.
</p>
<p>MPI –
это изначально <i>быстрый</i> инструмент.
Для повышения скорости в нем используются
приемы, о которых прикладные программисты
зачастую просто не задумываются.
Например, встроенная буферизация
позволяет избежать задержек при отправке
данных – управление в передающую ветвь
возвращается немедленно, даже если
ветвь-получатель еще не подготовилась
к приему. MPI использует многопоточность
(multi-threading), вынося большую часть своей
работы в потоки (threads) с низким приоритетом.
Буферизация и многопоточность сводят
к минимуму негативное влияние неизбежных
простоев при пересылках на производительность
прикладной программы. На передачу данных
типа <q>один-всем</q> оптимизированные
процедуры MPI затрачивают время,
пропорциональное не числу участвующих
ветвей, а логарифму этого числа и так
далее... И все это скрупулезно отлажено
и протестировано! 
</p>
<p>MPI
скрывает от нас подробности реализации
нижележащего транспортного уровня
(разделяемая память, встроенная сеть
или локальная сеть). Перенос программ
отныне не требует переписывания и
повторной отладки. Незаменимое качество
для программы, которой предстоит
пользоваться широкому кругу людей.
Следует учитывать и то обстоятельство,
что уже появляются машины, на которых
из средств межпрограммного взаимодействия
есть только MPI – и ничего более. 
</p>
<p><em><b>Отладка.</b></em>
Перечисление, иллюстрации и способы
избежания типовых ошибок, допускаемых
при параллельном программировании -
тема отдельного трактата. Сразу следует
оговориться, что панацеи от них пока не
придумано - иначе о таковой тотчас стало
бы общеизвестно. MPI ни от одной из типовых
ошибок (кроме <q>нарушения ордера</q>)
не страхует – но он уменьшает вероятность
их совершить! 
</p>
<p>Функции
работы с разделяемой памятью и с
семафорами слишком атомарны, примитивны.
Вероятность без ошибки реализовать с
их помощью нужное программе действие
стремительно уменьшается с ростом
количества инструкций; вероятность
найти ошибку путем отладки близка к
нулю, потому что отладочное средство
привносит задержку в выполнение одних
ветвей, и тем самым позволяет нормально
работать другим, (ветви перестают
конкурировать за совместно используемые
данные). 
</p>
<p>Программа,
использующая MPI, легче отлаживается
(сужается простор для совершения
стереотипных ошибок параллельного
программирования) и быстрее переносится
на другие платформы (в идеале, простой
перекомпиляцией). 
</p>
<p><em><b>Сравнение
с PVM.</b></em> MPI – не первая
попытка создания стандарта на обмен
сообщениями между ветвями параллельного
приложения. PVM – это изначально
исследовательский проект. Его несомненными
достоинствами являются простота
использования (в теории) и почтенный
возраст – он появился на свет в 1989 году,
и, таким образом, на 6 лет старше MPI. Надо
признать, по сравнению с PVM, MPI – это
<i><em>сложный</em></i> пакет. То, что в PVM
реализовано одним-единственным способом,
в MPI может быть сделано несколькими, про
которые говорится: способ А прост в
использовании, но не очень эффективен;
способ Б сложнее, но эффективнее; а
способ В сложнее и эффективнее <i>при
определенных условиях</i>. Изложение в
доступных через Интернет учебниках
рассчитано на профессиональных
программистов, а не на прикладников,
потому что упорядочено по областям
применения (способы А, Б и В в одной
главе), а не по уровням сложности (способ
А в первой главе, способ Б в следующей,
способ В и вовсе вынесен в приложения).
</p>
<p>Более
детальную информацию, как-то: спецификацию,
учебники и различные реализации MPI можно
найти на сервере <b>NetLib</b>. Первоисточником
для данного пособия является книга
<q>MPI: The complete reference</q> издательства MIT
Press. 
</p>
<p><i><em><b>Соглашения
о терминах.</b></em></i> Параллельное приложение
состоит из нескольких <em>ветвей</em>, или
<em>процессов</em>, или <em>задач</em>, выполняющихся
одновременно. Разные процессы могут
выполняться как на разных процессорах,
так и на одном и том же - для
программы это роли не играет, поскольку
в обоих случаях механизм обмена данными
одинаков. Процессы обмениваются друг
с другом данными в виде <em>сообщений</em>.
Сообщения проходят под  <em>идентификаторами</em>,
которые позволяют программе и библиотеке
связи отличать их друг от друга. Для
совместного проведения тех или иных
расчетов процессы внутри приложения
объединяются в <em>группы</em>. Каждый
процесс может узнать у библиотеки связи
свой номер внутри группы, и, в зависимости
от номера приступает к выполнению
соответствующей части расчетов. 
</p>
<p>Термин
<q>процесс</q> используется также в
Юниксе, и здесь нет путаницы: в MPI ветвь
запускается и работает как обычный
процесс Юникса, связанный через MPI с
остальными процессами, входящими в
приложение. В остальном процессы следует
считать изолированными друг от друга:
у них разные области кода, стека и данных.
</p>
<p><em><b>Особенность
MPI</b></em>: понятие <em>области связи</em>
(<em>communication domains</em>). При запуске приложения
все процессы помещаются в создаваемую
для приложения общую область связи. При
необходимости они могут создавать новые
области связи на базе существующих. Все
области связи имеют независимую друг
от друга нумерацию процессов. Программе
пользователя в распоряжение предоставляется
<em>коммуникатор</em>  – описатель
области связи. Многие функции MPI имеют
среди входных аргументов коммуникатор,
который ограничивает сферу их действия
той областью связи, к которой он
прикреплен. Для одной области связи
может существовать несколько коммуникаторов
таким образом, что приложение будет
работать с ней как с несколькими разными
областями. В исходных текстах примеров
для MPI часто используется идентификатор
<b>MPI_COMM_WORLD</b>. Это название коммуникатора,
создаваемого библиотекой автоматически.
Он описывает стартовую область связи,
объединяющую все процессы приложения.
</p>
<p><i><em><b>Категории
функций: блокирующие, локальные,
коллективные.</b></em></i>  Руководство по
MPI особо подчеркивается
принадлежность описываемой функции к
той или иной категории: 
</p>
<p><em>Блокирующие</em>
– останавливают (блокируют) выполнение
процесса до тех пор, пока производимая
ими операция не будет выполнена.
Неблокирующие функции возвращают
управление немедленно, а выполнение
операции продолжается в фоновом режиме;
за завершением операции надо проследить
особо. Неблокирующие функции возвращают
квитанции (<q>requests</q>), которые погашаются
при завершении. До погашения квитанции
с переменными и массивами, которые были
аргументами неблокирующей функции,
<b>ничего делать нельзя</b>. 
</p>
<p><em>Локальные</em>
– не инициируют пересылок данных между
ветвями. Большинство информационных
функций является локальными, т.к. копии
системных данных уже хранятся в каждой
ветви. Функция передачи MPI_Send и функция
синхронизации MPI_Barrier не являются
локальными, поскольку производят
пересылку. Следует заметить, что, к
примеру, функция приема MPI_Recv (парная
для MPI_Send) является локальной: она всего
лишь пассивно ждет поступления данных,
ничего не пытаясь сообщить другим
ветвям. 
</p>
<p><em>Коллективные</em>
– должны быть вызваны <b>всеми</b>
ветвями-абонентами того коммуникатора,
который передается им в качестве
аргумента. Несоблюдение для них этого
правила приводит к ошибкам на стадии
выполнения программы (как правило, к
зависанию). 
</p>
<p>
<i><em><b>Принятая в MPI нотация записи.</b></em></i>
Регистр букв: важен в Си, не играет
роли в Фортране. 
</p>
<p>Все идентификаторы начинаются с префикса <q>MPI_</q>. Это правило без исключений.
Не рекомендуется заводить пользовательские
идентификаторы, начинающиеся с этой
приставки, а также с приставок <q>MPID_</q>,
<q>MPIR_</q> и <q>PMPI_</q>, которые используются
в служебных целях. 
</p>
<p>Если
идентификатор сконструирован из
нескольких слов, слова в нем разделяются
подчерками: MPI_Get_count, MPI_Comm_rank. Иногда,
однако, разделитель не используется:
MPI_Sendrecv, MPI_Alltoall. 
</p>
<p>Порядок
слов в составном идентификаторе
выбирается по принципу <q>от общего к частному</q>: сначала префикс <q>MPI_</q>,
потом название категории ( Type, Comm, Group,
Attr, Errhandler и т.д.), потом название операции
( MPI_Errhandler_create, MPI_Errhandler_set, ...). Наиболее
часто употребляемые функции выпадают
из этой схемы: они имеют <q>анти-методические</q>,
но короткие и стереотипные названия,
например MPI_Barrier, или MPI_Unpack. 
</p>
<p>Имена
констант (и неизменяемых пользователем
переменных) записываются полностью
заглавными буквами: MPI_COMM_WORLD, MPI_FLOAT. В
именах функций первая за префиксом
буква - заглавная, остальные маленькие:
MPI_Send, MPI_Comm_size. 
</p>
<p>Такая
нотация по первому времени может
показаться непривычной, однако ее
использование быстро входит в навык.
Надо признать, что она делает исходные
тексты более читабельными. 
</p>
<p>Заголовочные
файлы: Необходимы для каждой программы
(процедуры), которая использует вызовы
функций MPI. 
</p>


<p><b>Общая структура MPI программы</b></p>
<ul>
<li>Подключение заголовочного файла MPI</li>
<li>...</li>
<li>Инициализация MPI коллектива</li>
<li>...</li>
<li>Делаем работу и осуществляем вызовы ппроцедур для передачи ссобщений</li>
<li>...</li>
<li>Разрушение коллектива процессов MPI</li>
</ul>

<p>
<b>Коммуникаторы и группы.</b> MPI использует
объекты называемые коммуникаторами и
группами для определения коллектива
процессов, которые могут иметь коммуникации
друг с другом. Большинство процедур MPI
требуют в качестве одного из аргументов
определения коммуникатора. 
</p>
<p>Коммуникаторы и группы будут рассмотрены подробнее
дальше. Сейчас стоит определить лишь
один предопределенный коммуникатор –
MPI_COMM_WORLD,
который существует всегда в программе
и включает в себя все процессы MPI.
</p>

<p><b>Ранг.</b> Внутри коммуникатора, каждый
процесс имеет свой собственный уникальный,
целочисленный идентификатор, назначенный
системой при инициализации коллектива
процессов. Процессы в коммуникаторе
нумеруются последовательно, начиная с
нуля. Идентификаторы используются в
программе, чтобы определить источник
и приемник сообщений. Часто используются
в условных конструкциях, чтобы управлять
выполнением программы (если ID=0
делаем это, а если ID=1
делаем то). 
</p>

<h1 id="__RefHeading___Toc5075829">Обрамляющие функции. Начало и завершение.</h1>

<p>Существует несколько функций, которые используются
в любом, даже самом коротком приложении
MPI. Занимаются они не столько собственно
передачей данных, сколько ее обеспечением:
</p>

<p><b>MPI_Init</b>: Инициализация библиотеки,
создание первоначального коллектива
MPI_COMM_WORLD. Одна из первых инструкций в
функции main (главной функции приложения).
Она получает адреса аргументов, стандартно
получаемых самой main от операционной
системы и хранящих параметры командной
строки. В конец командной строки программы
MPI-загрузчик добавляет ряд информационных
параметров, которые требуются MPI_Init. Эта
функция должна быть вызвана в любой
программе MPI, <i>перед</i> любой другой
функцией MPI и вызвана лишь <i>однажды</i>.
Для C программ, MPI_Init
может быть использована для передачи
аргументов командной строки всем
процессам, хотя это не
требуется по стандарту и зависит от
реализации MPI.
</p>
<p style="text-indent: 1.91cm; margin-bottom: 0cm"><b>int MPI_Init</b><b>
</b><b>(int argc,  char***argv);</b></p>
<p><br/>
</p>
<p>
<b>MPI_Initialized</b>: Определяет, была ли вызвана
процедура MPI_Init и возвращает логическое
значение. MPI требует, чтобы MPI_Init была
вызвана для каждого процесса и только
один раз. Это может составлять проблему
при наличии нескольких программных
модулей, которые используют MPI и при
необходимости вызывают MPI_Init. MPI_Initialized
решает эту проблему. Это единственная
функция MPI, которую можно вызывать до
MPI_Init.</p>
<p>
<b>int MPI_Initialized (int *flag)</b></p>
<p>
<b>MPI_INITIALIZED(flag,ierr) </b>
</p>
<p><b>MPI_Abort</b>:
Вызов MPI_Abort из любой задачи принудительно
завершает работу <i>всех</i> задач,
подсоединенных к заданной области
связи (коммуникатору). Если указан
описатель MPI_COMM_WORLD, будет завершено все
приложение (все его задачи) целиком,
что, по-видимому, и является наиболее
правильным решением. Рекомендуется в
качестве коммуникатора всегда указывать
MPI_COMM_WORLD, хотя в большинстве реализаций
MPI аварийно заканчивают работу <i>все</i>
процессы независимо от указанного
коммуникатора. Следует помнить, что
стандартные функции exit,
abort, terminate,
stop могут завершить работу
лишь одного процесса. Выпадение из
коллектива одного процесса может
привести к зависанию остальных. Следует
всегда использовать только MPI_Abort для
аварийного завершения работы. 
</p>
<p>
<b>int MPI_Abort (MPI_Comm comm, int errorcode)</b></p>
<p>
<b>MPI</b><b>_</b><b>Finalize</b>:
Нормальное завершение работы коллектива
процессов. Эта функция должна быть
последней процедурой MPI,
вызванной в любой MPI
программе – никакие другие MPI
процедуры не могут быть вызваны после
нее. Настоятельно рекомендуется не
забывать вписывать эту инструкцию перед
возвращением из программы, то есть в
конце головной программы main (program).</p>
<p>
<b>int MPI_Finalize (void)</b></p>
<p style="margin-left: -0.64cm; text-indent: 0cm; margin-bottom: 0cm">
<br/>
</p>
<p>
<b>MPI</b><b>_</b><b>Comm</b><b>_</b><b>size</b><b>:
</b>Определяет число процессов в заданном коммуникаторе (области связи).
При использовании с коммуникатором MPI_COMM_WORLD возвращает число процессов, присоединенных к программе. 
</p>
<p>
<b>int MPI_Comm_size (MPI_Comm comm, int *size)</b></p>
<p>
<br/>
</p>
<p>
<b>MPI</b><b>_</b><b>Comm</b><b>_</b><b>rank</b><b>:</b>
Определяет порядковый номер процесса в коммуникаторе.
Первоначально, каждый процесс имеет уникальный номер от 0..N-1, где N
–  число процессов в коммуникаторе MPI_COMM_WORLD.
Если процесс становится связанным с другим коммуникатором,
то в нем он будет иметь другой уникальный номер в пределах новой области связи.</p>

<p>
<b>int MPI_Comm_rank (MPI_Comm comm, int *rank)</b></p>
<p>
<br/>
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Пример использования обрамляющих функций:</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    int numtasks, rank, rc;

    rc = MPI_Init(&argc,&argv);
    if(rc != 0) {
    printf("Error starting MPI program. Terminating.\n");

    MPI_Abort(MPI_COMM_WORLD, rc);
    }
    MPI_Comm_size(MPI_COMM_WORLD,&numtasks);
    MPI_Comm_rank(MPI_COMM_WORLD,&rank);
    printf("Number of tasks= %d My rank= %d\n", numtasks, rank);
    MPI_Finalize();
    return 0;
}
]]></div>

<h1 id="__RefHeading___Toc5075830">Простейшие процедуры передачи сообщений типа <q>точка-точка</q>.</h1>

<p>Это самый простой тип связи между задачами:
одна ветвь вызывает функцию передачи данных, а другая – функцию приема.
Формат и списки аргументов MPI процедур типа «точка-точка» приведены в таблице:
</p>

 <table>
  <tr>
   <td>Посылка с блокировкой</td>
   <td>MPI_Send(buffer, count, type, dest, tag, comm)</td>
  </tr>
  <tr>
   <td>Посылка без блокировки</td>
   <td>MPI_Isend(buffer, count, type, dest, tag, comm, request)</td>
  </tr>
  <tr>
   <td>Прием с блокировкой</td>
   <td>MPI_Recv(buffer, count, type, source, tag, comm, status)</td>
  </tr>
  <tr>
   <td>Прием без блокировки</td>
   <td>MPI_Irecv(buffer, count, type, source, tag, comm, request)</td>
  </tr>
 </table>

<p>В MPI это выглядит, например, так: 
</p>
<p>Задача №0 передает 5 целых чисел: 
</p>
<p>int buf[10];</p>
<p>MPI_Send(buf, 5, MPI_INT, 1, 0, MPI_COMM_WORLD);
</p>
<p>Задача №1 принимает сообщение и определяет его длину:
</p>

<div class="lang-cpp"><![CDATA[
int buf[10],len;
MPI_Status status;
MPI_Recv(buf, 10, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);
MPI_Get_count(&status, MPI_INT, &len);
]]></div>

<p>Аргументы функций:</p>

<p><em>Адрес буфера</em>, из которого в задаче 1 берутся,
а в задаче 2 помещаются данные. Помните,
что наборы данных у каждой задачи свои,
поэтому, например, используя одно и то
же имя массива в нескольких задачах, Вы
указываете не одну и ту же область
памяти, а разные, никак друг с другом не
связанные. 
</p>
<p><em>Размер буфера</em>. Задается <b>не в байтах</b>, а в
количестве ячеек. Для MPI_Send указывает,
сколько ячеек требуется передать (в
примере передаются 5 чисел). В MPI_Recv
означает максимальную емкость приемного
буфера. Если фактическая длина пришедшего
сообщения меньше – последние ячейки
буфера останутся нетронутыми, если
больше – произойдет ошибка времени
выполнения.</p>
<p><em>Тип ячейки буфера</em>. Из соображений
переносимости, MPI оперирует
массивами однотипных данных. Предопределены
стандартные типы данных, соответствующие типам данных C++.
Для описания базовых типов Си в MPI определены константы
MPI_INT, MPI_CHAR, MPI_DOUBLE и так далее, имеющие тип <b>MPI_Datatype</b>. Их названия образуются
префиксом <q>MPI_</q> и именем соответствующего
типа (int, char, double, ...), записанным заглавными
буквами. Пользователь может <q>регистрировать</q> в MPI свои собственные типы данных
(производные типы),
например структуры, после чего MPI сможет
обрабатывать их наравне с базовыми
типами. Процесс регистрации описывается
далее. Специальные MPI типы
MPI_BYTE и MPI_PACKED не относятся к
стандартным типам C.

</p>
<p><em>Номер
задачи</em>, с которой происходит обмен
данными. Все задачи внутри созданной
MPI группы автоматически нумеруются от
0 до (размер группы-1). В примере задача
0 передает задаче 1, задача 1 принимает
от задачи 0. При приеме номером источника
может быть MPI_ANY_SOURCE для
приема сообщения от любого процесса.

</p>
<p><em>Идентификатор
сообщения</em>. Произвольное неотрицательное
целое число, назначенное программистом,
чтобы уникально идентифицировать
сообщение. MPI стандарт гарантирует, что
целые числа 0-32767 можно использовать как
признаки, но большинство реализаций
позволяют больший диапазон значений.
Оно служит той же цели, что и, например,
расширение файла – задача-приемник: по
идентификатору определяет смысл принятой
информации; сообщения, пришедшие в
неизвестном порядке, может извлекать
из общего входного потока в нужном
алгоритму порядке. Хорошим тоном является
обозначение идентификаторов символьными
именами посредством операторов <q>#define</q>
или <q>const int</q>. Операции
Send/Receive, должны совпадать
по идентификаторам сообщения. Для приема
можно использовать MPI_ANY_TAG, чтобы получить
сообщение с любым идентификатором. 
</p>
<p><em>Описатель
области связи</em> (коммуникатор). Указывает
контекст связи (множество процессов),
относительно которых задаются номера
источника и приемника сообщений. Он
обязан быть одинаковым для MPI_Send и
MPI_Recv. Если программист не создает явно
новые коммуникаторы, обычно используется
предопределенный коммуникатор
MPI_COMM_WORLD. 
</p>
<p><em>Статус
завершения приема</em>. Содержит информацию
о принятом сообщении: его идентификатор,
номер задачи-передатчика, код завершения
и количество фактически пришедших
данных. В C этот аргумент – указатель
на предопределенную структуру MPI_Status
(с полями MPI_SOURCE и MPI_TAG).
</p>
<p>С одной
стороны, мы передаем в MPI_Recv номер задачи,
от которой ждем сообщение, и его
идентификатор; а с другой – получаем
их от MPI в структуре <b>status</b>? Это сделано
потому, что MPI_Recv может быть вызвана с
аргументами-<b>джокерами</b> MPI_ANY_TAG
 и MPI_ANY_SOURCE (<q>принимай что угодно/от кого угодно</q>), и после
такого приема данных программа узнает
фактические номер/идентификатор, читая
поля MPI_SOURCE и MPI_TAG из структуры status. 
</p>
<p>Поле
MPI_ERROR, как правило, проверять необязательно
– обработчик ошибок, устанавливаемый
MPI по умолчанию, в случае сбоя завершит
выполнение программы до возврата из
MPI_Recv. Таким образом, после возврата из
MPI_Recv поле status.MPI_ERROR может быть равно
только MPI_SUCCESS.</p>
<p>Тип
MPI_Status не содержит поля, в которое
записывалась бы фактическая длина
пришедшего сообщения. Длина принятого
сообщения определяет потом с помощью
функции MPI_Get_count:</p>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
int MPI_Get_count( MPI_Status *status,
MPI_Datatype datatype, int *count)

</p>
<ul>
 <li><p><em>status</em> - параметры принятого сообщения</p></li>
 <li><p><em>datatype</em> - тип элементов принятого сообщения</p></li>
 <li><p>OUT <em>count</em> - число элементов сообщения</p></li>
</ul>
<p>Например:</p>

<div class="lang-cpp"><![CDATA[
MPI_Status status;
int count;
MPI_Recv(... ,  MPI_INT, ... , &status);
MPI_Get_count(&status, MPI_INT, &count);
/* ... теперь count содержит количество принятых ячеек */
]]></div>

<p>
Обратите внимание, что аргумент-описатель
типа у MPI_Recv и MPI_Get_count должен быть
одинаковым, иначе, в зависимости от
реализации: в count вернется неверное
значение или произойдет ошибка времени
выполнения. 
</p>
<p>
<em>Запрос на завершение отложенной
блокировки.</em> Используется для не
блокируемых операций send/receive. Поскольку
не блокируемая операция
может вернуть управление прежде, чем
будет завершена операция передачи
данных, система возвращает уникальный <q>номер запроса</q>. Программист
использует этот назначенный системой
дескриптор позже (в процедурах типа
WAIT)
для завершения отложенного блокирования.
В C этот аргумент – указатель на
предопределенную структуру MPI_Request.</p>

<table>
<caption>Типы данных MPI</caption>
<thead>
<tr>
<th>тип MPI</th>
<th>тип данных</th>
</tr>
</thead>
<tbody>
<tr><td>MPI_CHAR</td><td>signed char</td></tr>
<tr><td>MPI_SHORT</td><td>signed short int</td></tr>
<tr><td>MPI_INT</td><td>signed int</td></tr>
<tr><td>MPI_LONG</td><td>signed long int</td></tr>
<tr><td>MPI_UNSIGNED_CHAR</td><td>unsigned char</td></tr>
<tr><td>MPI_UNSIGNED_SHORT</td><td>unsigned short int</td></tr>
<tr><td>MPI_UNSIGNED</td><td>unsigned int</td></tr>
<tr><td>MPI_UNSIGNED_LONG</td><td>unsigned long int</td></tr>
<tr><td>MPI_FLOAT</td><td>float</td></tr>
<tr><td>MPI_DOUBLE</td><td>double</td></tr>
<tr><td>MPI_LONG_DOUBLE</td><td>long double</td></tr>
<tr><td>MPI_BYTE</td><td>8 binary digits</td></tr>
<tr><td>MPI_PACKED</td><td>data packed or unpacked with MPI_Pack()/ MPI_Unpack</td></tr>
</tbody>
</table>

<p>Наиболее используемые функции MPI для передачи сообщений с блокировкой:

</p>
<p><br/>
</p>
<p>
int MPI_Send(void* buf, int count,
MPI_Datatype datatype, int dest, int msgtag, MPI_Comm comm) 
</p>
<ul>
 <li><p><em>buf</em> - адрес начала буфера посылки сообщения</p></li>
 <li><p><em>count</em> - число передаваемых элементов в сообщении</p></li>
 <li><p><em>datatype</em> - тип передаваемых элементов</p></li>
 <li><p><em>dest</em> - номер процесса-получателя</p></li>
 <li><p><em>msgtag</em> - идентификатор сообщения</p></li>
 <li><p><em>comm</em> - коммуникатор</p></li>
</ul>
<p>Базисная
операция посылки данных. Блокирующая
посылка сообщения с идентификатором
<em>msgtag</em>, состоящего из <em>count</em> элементов
типа <em>datatype</em>, процессу с номером <em>dest</em>.
Все элементы сообщения расположены
подряд в буфере <em>buf</em>. Значение <em>count</em>
может быть нулем. Тип передаваемых
элементов <em>datatype</em> должен указываться
с помощью предопределенных констант
типа. Разрешается передавать сообщение
самому себе. 
</p>
<p>Блокировка
гарантирует корректность повторного
использования всех параметров после
возврата из подпрограммы: функция
возвращает управление только после
того, как прикладной буфер
процессора-отправителя свободен для
использования в других целях. 
</p>
<p>Эта
функция может быть реализована различно
на разных системах. Стандарт MPI разрешает
использование системного буфера, но не
требует этого. Некоторые реализации
могут фактически использовать синхронную
передачу (обсуждается ниже). Выбор
способа осуществления этой гарантии:
копирование в промежуточный буфер или
непосредственная передача процессу
<em>dest</em>, остается за MPI. Возврат из
подпрограммы <em>MPI_Send</em> не означает ни
того, что сообщение уже передано процессу
<em>dest</em>, ни того, что сообщение покинуло
процессорный элемент, на котором
выполняется процесс, выполнивший
<em>MPI_Send</em>. 
</p>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
int MPI_Recv(void* buf, int count,
MPI_Datatype datatype, int source, int msgtag, MPI_Comm comm,
MPI_Status *status) 
</p>
<ul>
 <li><p>OUT <em>buf</em> - адрес начала буфера приема сообщения</p></li>
 <li><p><em>count</em> - максимальное число элементов в принимаемом сообщении</p></li>
 <li><p><em>datatype</em> - тип элементов принимаемого сообщения</p></li>
 <li><p><em>source</em> - номер процесса-отправителя</p></li>
 <li><p><em>msgtag</em> - идентификатор принимаемого сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>status</em> - параметры принятого сообщения</p></li>
</ul>
<p>Прием
сообщения с идентификатором <em>msgtag</em>
от процесса <em>source</em> с блокировкой.
Число элементов в принимаемом сообщении
не должно превосходить значения <em>count</em>.
Если число принятых элементов меньше
значения <em>count</em>, то гарантируется, что
в буфере <em>buf</em> изменятся только
элементы, соответствующие элементам
принятого сообщения.</p>
<p>Блокировка
гарантирует, что после возврата из
подпрограммы все элементы сообщения
приняты и расположены в буфере <em>buf</em>.
</p>
<p>В
качестве номера процесса-отправителя
можно указать предопределенную константу
<em>MPI_ANY_SOURCE</em> - признак того, что подходит
сообщение от любого процесса. В качестве
идентификатора принимаемого сообщения
можно указать константу <em>MPI_ANY_TAG</em> -
признак того, что подходит сообщение с
любым идентификатором. 
</p>
<p>Если
процесс посылает два сообщения другому
процессу, и оба эти сообщения соответствуют
одному и тому же вызову <em>MPI_Recv</em>, то
первым будет принято то сообщение,
которое было отправлено раньше.</p>
<p>
<br/>
</p>
<p>
<b>MPI_Ssend</b><b>. </b>Синхронная
блокируемая посылка. Аналогична
<em>MPI_Send</em><em>,</em>
за исключением того, что процесс-отправитель
гарантированно дождется, пока
процесс-получатель примет сообщение,
и только потом вернет управление.</p>
<p>
<br/>
</p>
<p>
<b>MPI_Bsend</b><b>. </b>Буферизуемая посылка с блокировкой.
Аналогична <em>MPI_Send</em><em>,</em>
за исключением того, что позволяет
пользователю работать с собственной
буферизацией. Используется
при недостатке места в системном буфере.
Возвращает управление после того, как
сообщение будет скопировано из
программного буфера в назначенный
буфер. Должна использоваться вместе с
MPI_BUFFER_ATTACH. 
</p>
<p>
<br/>
</p>
<p>
<b>MPI_Buffer_attach </b><b>и
</b><b>MPI_Buffer_detach</b>.
Используются для назначения
(освобождения) буфера
сообщений, используемого функцией
MPI_Bsend. Аргумент «size»
определяет число байт –
размер буфера. Только
один буфер может быть назначен
для процесса. 
</p>
<p>
<b>int
MPI_Buffer_attach (void *buffer, int size)</b></p>
<p>
<b>int
MPI_Buffer_detach (void *buffer, int size)</b></p>
<p>
<br/>
</p>
<p>
<b>MPI_Rsend</b><b>. </b>Небуферизуемая
посылка с блокировкой.
Аналогична <em>MPI_Send</em><em>,</em>
за исключением того, что ее рекомендуется
использоваться, только если
программист уверен,
что соответствующий
получатель
 уже готов к приему.

</p>
<p>
Некоторые конструкции с приемом-передачей применяются очень часто:
</p>

<div class="lang-cpp"><![CDATA[
/* Обмен данными с соседями по группе
   (в группе четное количество ветвей!): */
MPI_Comm_size(MPI_COMM_WORLD, &size);
MPI_Comm_rank(
MPI_COMM_WORLD, &rank);
if(rank % 2 ) {
    /* Ветви с четными номерами передают следующим нечетным ветвям */
    MPI_Send(...,( rank+1) % size,...);
    /* а потом принимают от предыдущих */
    MPI_Recv(...,( rank+size-1) % size ,...);
} else {
    /* Нечетные ветви поступают наоборот:
       сначала принимают от предыдущих ветвей */
    MPI_Recv(..., ( rank-1) % size ,...);
    /* потом передают следующим. */
    MPI_Send(..., ( rank+1) % size ,...);
}

/* Посылка данных и получение подтверждения: */
MPI_Send(..., anyRank ,...);   /* Посылаем данные */
MPI_Recv(..., anyRank ,...);   /* Принимаем подтверждение */ 
]]></div>

<p>Ситуация
настолько распространенная, что в MPI
специально введены две функции,
осуществляющие одновременно посылку
одних данных и прием других. 
</p>
<p>int MPI_Sendrecv( void *sbuf, int scount,
MPI_Datatype stype, int dest, int stag, void *rbuf, int rcount,
MPI_Datatype rtype, int source, MPI_Datatype rtag, MPI_Comm comm,
MPI_Status *status) 
</p>
<ul>
 <li><p><em>sbuf</em> - адрес начала буфера посылки сообщения</p></li>
 <li><p><em>scount</em> - число передаваемых элементов в сообщении</p></li>
 <li><p><em>stype</em> - тип передаваемых элементов</p></li>
 <li><p><em>dest</em> - номер процесса-получателя</p></li>
 <li><p><em>stag</em> - идентификатор посылаемого сообщения</p></li>
 <li><p>OUT <em>rbuf</em> - адрес начала буфера приема сообщения</p></li>
 <li><p><em>rcount</em> - число принимаемых элементов сообщения</p></li>
 <li><p><em>rtype</em> - тип принимаемых элементов</p></li>
 <li><p><em>source</em> - номер процесса-отправителя</p></li>
 <li><p><em>rtag</em> - идентификатор принимаемого сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>status</em> - параметры принятого сообщения</p></li>
</ul>
<p>Данная
операция объединяет в едином запросе
посылку и прием сообщений. Один вызов
MPI_Sendrecv проделывает те же действия, для
которых в первом фрагменте требуется
блок IF-ELSE с четырьмя вызовами.
Принимающий и отправляющий процессы
могут являться одним и тем же процессом.
</p>
<p>Сообщение,
отправленное операцией <em>MPI_Sendrecv</em>,
может быть принято обычным образом
(например, MPI_Recv),
и точно также операция <em>MPI_Sendrecv</em> может
принять сообщение, отправленное обычной
операцией <em>MPI_Send</em>. Буфера приема и
посылки обязательно должны быть
различными. Блокировка, пока
send-буфер не освободится
и recv-буфер не будет готов.
Следует учесть, что и прием, и передача
используют один и тот же коммуникатор,
порядок приема и передачи данных
MPI_Sendrecv выбирает автоматически;
гарантируется, что автоматический выбор
не приведет к <q>клинчу</q>.
</p>
<p>int MPI_Sendrecv_replace(void *buf, int
count, MPI_Datatype stype, int dest, int stag, int source, int rtag,
MPI_Comm comm, MPI_Status *status)

</p>
<p><b>MPI_Sendrecv_replace</b>
помимо общего коммуникатора использует
еще и общий для приема-передачи буфер.
Не очень удобно, что параметр count получает
двойное толкование: это и количество
отправляемых данных, и предельная
емкость входного буфера. Показания к
применению: принимаемые данные должны
быть заведомо <i><em>не длиннее</em></i>
отправляемых, принимаемые и отправляемые
данные должны иметь одинаковый тип,
отправляемые данные затираются
принимаемыми. MPI_Sendrecv_replace так же
гарантированно не вызывает клинча. 
</p>
<p>Что
такое <i><em>клинч</em></i>? Дальше следует
краткая иллюстрация этой ошибки, очень
распространенной там, где для пересылок
используется разделяемая память. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Вариант
1: 
</p>
<p>    --
Ветвь 1 --          -- Ветвь 2 --</p>
<p>    Recv(
из ветви 2 )     Recv( из ветви 1 )</p>
<p>    Send(
в ветвь 2 )      Send( в ветвь 1 ) 
</p>
<p>Вариант
1 вызовет клинч, какой бы инструментарий
не использовался. Функция приема не
вернет управления до тех пор, пока не
получит данные, поэтому функция передачи
не может приступить к отправке данных;
поэтому функция приема... и так далее. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Вариант
2: 
</p>
<p>    --
Ветвь 1 --          -- Ветвь 2 --</p>
<p>    Send(
в ветвь 2 )      Send( в ветвь 1 )</p>
<p>    Recv(
из ветви 2 )     Recv( из ветви 1 ) 
</p>
<p>Вариант
2 вызовет клинч, если функция передачи
возвращает управление только после
того, как данные попали в пользовательский
буфер на приемной стороне. Скорее всего,
именно так и возьмется реализовывать
передачу через разделяемую память/семафоры
программист-проблемщик. 
</p>
<p>Однако
при использовании MPI зависания во втором
варианте не произойдет! MPI_Send, если на
приемной стороне нет готовности (не
вызван MPI_Recv), не станет ее дожидаться,
а положит данные во временный буфер и
вернет управление программе <i><em>немедленно</em></i>.
Когда MPI_Recv будет вызван, данные он
получит не напрямую из пользовательского
буфера, а из промежуточного системного.
Буферизация – дело громоздкое – может
быть, и не всегда сильно экономит время
(особенно на SMP-машинах), зато повышает
надежность: делает программу более
устойчивой к ошибкам программиста. 
</p>
<p>MPI_Sendrecv
и MPI_Sendrecv_replace также делают программу
более устойчивой: с их использованием
программист лишается возможности
перепутать варианты 1 и 2. 
</p>
<p>Пример
передач сообщений с блокировкой: Задача
0 шлет информацию задаче
1 и ожидает ответа. 
</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>

int main(int argc, char **argv) {
    int numtasks, rank, dest, source, rc, tag=1;
    char inmsg, outmsg='x';
    MPI_Status Stat;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if (rank == 0) {
      dest = source = 1;
      MPI_Send(&outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
      MPI_Recv(&inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &Stat);
    } else if (rank == 1) {
      dest = source = 0;
      MPI_Recv(&inmsg, 1, MPI_CHAR, source, tag, MPI_COMM_WORLD, &Stat);
      MPI_Send(&outmsg, 1, MPI_CHAR, dest, tag, MPI_COMM_WORLD);
    }
    MPI_Finalize();
    return 0;
}
]]></div>


<h1 id="__RefHeading___Toc5075831">Не блокируемые  процедуры передачи сообщений типа <q>точка-точка</q>.</h1>

<p>Наиболее широко используемые функции MPI для неблокируемых передач сообщений:</p>

<p>
int MPI_Isend(void *buf, int count, MPI_Datatype datatype, int dest, int msgtag, MPI_Comm comm,
MPI_Request *request) 
</p>
<ul>
 <li><p><em>buf</em> - адрес начала буфера посылки сообщения</p></li>
 <li><p><em>count</em> - число передаваемых элементов в сообщении</p></li>
 <li><p><em>datatype</em> - тип передаваемых элементов </p></li>
 <li><p><em>dest</em> - номер процесса-получателя</p></li>
 <li><p><em>msgtag</em> - идентификатор сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>request</em> - идентификатор асинхронной передачи</p></li>
</ul>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0cm">
Передача сообщения, аналогичная <em>MPI_Send</em>,
однако возврат из подпрограммы происходит
сразу после инициализации процесса
передачи без ожидания обработки всего
сообщения, находящегося в буфере <em>buf
</em>(возвращает управление
немедленно, без ожидания отправки
сообщения или его копирования системный
буфер). Это означает, что нельзя
повторно использовать данный буфер для
других целей без получения дополнительной
информации о завершении данной посылки.
Дескриптор запроса <em>request</em>
возвращается для последующей отложенной
блокировки. Окончание процесса
передачи (т.е. того момента, когда можно
переиспользовать буфер <em>buf</em> без
опасения испортить передаваемое
сообщение) можно определить с помощью
параметра <em>request</em> и процедур <em>MPI_Wait</em>
и <em>MPI_Test</em>. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Сообщение,
отправленное любой из процедур <em>MPI_Send</em>
и <em>MPI_Isend</em>, может быть принято любой
из процедур <em>MPI_Recv</em> и <em>MPI_Irecv</em>. 
</p>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
int MPI_Irecv(void *buf, int count,
MPI_Datatype datatype, int source, int msgtag, MPI_Comm comm,
MPI_Request *request) 
</p>
<ul>
 <li><p>OUT <em>buf</em> - адрес начала буфера приема сообщения</p></li>
 <li><p><em>count</em> - максимальное число элементов в принимаемом сообщении</p></li>
 <li><p><em>datatype</em> - тип элементов принимаемого сообщения</p></li>
 <li><p><em>source</em> - номер процесса-отправителя</p></li>
 <li><p><em>msgtag</em> - идентификатор принимаемого сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>request</em> - идентификатор асинхронного приема сообщения</p></li>
</ul>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
Прием сообщения, аналогичный <em>MPI_Recv</em>,
однако возврат из подпрограммы происходит
сразу после инициализации процесса
приема без ожидания получения сообщения
в буфере <em>buf</em>. Возвращает
управление немедленно, без ожидания
получения сообщения или его копирования
из системного буфера. Дескриптор запроса
возвращается для последующей отложенной
блокировки и получения статуса сообщения.
Программа не должна изменять прикладной
буфер, пока последующие запросы к
MPI_WAIT или MPI_TEST не укажут, что неблокируемый
прием закончился.</p>
<p>
<b>int
MPI_Issend (void *buf, int count, MPI_Datatype datatype, int dest,
int tag, MPI_Comm comm, MPI_Request *request)</b></p>
<p style="margin-top: 0.21cm; margin-bottom: 0cm">
Неблокируемая синхронная передача.
Аналогично MPI_Isend, за
исключением того, что MPI_Wait
или MPI_Test будут ожидать
приема этого сообщения процессом-приемником.

</p>
<p style="margin-top: 0.21cm; margin-bottom: 0cm">
int MPI_Ibsend (void *buf, int count, MPI_Datatype datatype,
int dest, int tag, MPI_Comm comm, MPI_Request *request) 
</p>
<p style="margin-top: 0.21cm; margin-bottom: 0cm">
Неблокируемая буферизуемая передача.
Аналогично MPI_Bsend, за
исключением того, что MPI_Wait
или MPI_Test будут ожидать
приема этого сообщения процессом-приемником.
Должна использоваться только с
MPI_Buffer_attach. 
</p>

<p><b>int
MPI_Irsend (void *buf, int count, MPI_Datatype datatype, int dest,
int tag, MPI_Comm comm, MPI_Request *request) </b>
</p>

<p>Неблокируемая буферизуемая передача.
Аналогично MPI_Rsend, за
исключением того, что MPI_Wait
или MPI_Test будут ожидать
приема этого сообщения процессом-приемником.
Должна использоваться, только если
пользователь уверен,
что соответствующий
приемник
уже готов.

</p>
<p><br/>
</p>
<p>
<em><b>Функции типа </b></em><em><b>WAIT</b></em><em><b>:</b></em>
завершают отложенную блокировку, ожидая
завершения неблокируемой передачи. Для
нескольких неблокируемых передач,
пользователь может ожидать их: все,
некоторые или первую завершенную.</p>
<p>int MPI_Wait( MPI_Request *request,
MPI_Status *status) 
</p>
<ul>
 <li><p><em>request</em> - идентификатор асинхронного приема или передачи</p></li>
 <li><p>OUT <em>status</em> - параметры сообщения</p></li>
</ul>
<p>
Ожидание завершения асинхронных процедур
<em>MPI_Isend</em> или <em>MPI_Irecv</em>, ассоциированных
с идентификатором <em>request</em>. В случае
приема, атрибуты и длину полученного
сообщения можно определить обычным
образом с помощью параметра <em>status</em>. 
</p>
<p>int MPI_Waitall( int count, MPI_Request
*requests, MPI_Status *statuses) 
</p>
<ul>
 <li><p><em>count</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>statuses</em> - параметры сообщений</p></li>
</ul>

<p>Выполнение процесса блокируется до тех
пор, пока все операции обмена,
ассоциированные с указанными
идентификаторами, не будут завершены.
Если во время одной или нескольких
операций обмена возникли ошибки, то
поле ошибки в элементах массива <em>statuses</em>
будет установлено в соответствующее
значение. 
</p>
<p>int MPI_Waitany( int count, MPI_Request
*requests, int *index, MPI_Status *status) 
</p>
<ul>
 <li><p><em>count</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>index</em> - номер завершенной операции обмена</p></li>
 <li><p>OUT <em>status</em> - параметры сообщений</p></li>
</ul>

<p>Выполнение процесса блокируется до тех
пор, пока какая-либо операция обмена,
ассоциированная с указанными
идентификаторами, не будет завершена.
Если несколько операций может быть
завершено, то случайным образом выбирается
одна из них. Параметр <em>index</em> содержит
номер элемента в массиве <em>requests</em>,
содержащего идентификатор завершенной
операции. 
</p>
<p>int MPI_Waitsome( int incount, MPI_Request
*requests, int *outcount, int *indexes, MPI_Status *statuses)

</p>
<ul>
 <li><p><em>incount</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>outcount</em> - число идентификаторов завершившихся операций обмена</p></li>
 <li><p>OUT <em>indexes</em> - массив номеров завершившихся операции обмена</p></li>
 <li><p>OUT <em>statuses</em> - параметры завершившихся сообщений</p></li>
</ul>

<p>Выполнение процесса блокируется до тех
пор, пока по крайней мере одна из операций
обмена, ассоциированных с указанными
идентификаторами, не будет завершена.
Параметр <em>outcount</em> содержит число
завершенных операций, а первые <em>outcount</em>
элементов массива <em>indexes</em> содержат
номера элементов массива <em>requests</em> с
их идентификаторами. Первые <em>outcount</em>
элементов массива <em>statuses</em> содержат
параметры завершенных операций. 
</p>

<p><em><b>Функции типа </b></em><em><b>TEST</b></em><em><b>:</b></em>
проверяют статус передач
с отложенной блокировкой.
Параметр <q>flag</q> возвращает логическое значение ИСТИНА, если операция завершилась.
Для нескольких неблокируемых передач, пользователь может проверять их: все, некоторые или первую завершенную.

</p>
<p>int MPI_Test( MPI_Request *request, int
*flag, MPI_Status *status) 
</p>
<ul>
 <li><p><em>request</em> - идентификатор асинхронного приема или передачи</p></li>
 <li><p>OUT <em>flag</em> - признак завершенности операции обмена</p></li>
 <li><p>OUT <em>status</em> - параметры сообщения</p></li>
</ul>

<p>Проверка завершенности асинхронных
процедур <em>MPI_Isend</em> или <em>MPI_Irecv</em>,
ассоциированных с идентификатором
<em>request</em>. В параметре <em>flag</em> возвращает
значение 1, если соответствующая операция
завершена, и значение 0 в противном
случае. Если завершена процедура приема,
то атрибуты и длину полученного сообщения
можно определить обычным образом с
помощью параметра <em>status</em>. 
</p>
<p>int MPI_Testall( int count, MPI_Request
*requests, int *flag, MPI_Status *statuses)

</p>
<ul>
 <li><p><em>count</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>flag</em> - признак завершенности операций обмена</p></li>
 <li><p>OUT <em>statuses</em> - параметры сообщений</p></li>
</ul>

<p>В параметре <em>flag</em> возвращает значение
<em>1</em>, если все операции, ассоциированные
с указанными идентификаторами, завершены
(с указанием параметров сообщений в
массиве <em>statuses</em>). В противном случае
возвращается <em>0</em>, а элементы массива
<em>statuses</em> неопределены. 
</p>
<p>int MPI_Testany(int count, MPI_Request
*requests, int *index, int *flag, MPI_Status *status)

</p>
<ul>
 <li><p><em>count</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>index</em> - номер завершенной операции обмена</p></li>
 <li><p>OUT <em>flag</em> - признак завершенности операции обмена</p></li>
 <li><p>OUT <em>status</em> - параметры сообщения</p></li>
</ul>

<p>Если к моменту вызова подпрограммы хотя
бы одна из операций обмена завершилась,
то в параметре <em>flag</em> возвращается
значение <em>1</em>, <em>index</em> содержит номер
соответствующего элемента в массиве
<em>requests</em>, а <em>status</em> - параметры сообщения.
</p>
<p>int MPI_Testsome( int incount, MPI_Request
*requests, int *outcount, int *indexes, MPI_Status *statuses)
</p>

<ul>
 <li><p><em>incount</em> - число идентификаторов</p></li>
 <li><p><em>requests</em> - массив идентификаторов асинхронного приема или передачи</p></li>
 <li><p>OUT <em>outcount</em> - число идентификаторов завершившихся операций обмена</p></li>
 <li><p>OUT <em>indexes</em> - массив номеров завершившихся операции обмена</p></li>
 <li><p>OUT <em>statuses</em> - параметры завершившихся операций</p></li>
</ul>

<p>Данная подпрограмма работает так же,
как и <em>MPI_Waitsome</em>, за исключением того,
что возврат происходит немедленно. Если
ни одна из указанных операций не
завершилась, то значение <em>outcount</em> будет
равно нулю. 
</p>

<p><em><b>Объединение запросов на взаимодействие.</b></em>
Процедуры данной группы позволяют
снизить накладные расходы, возникающие
в рамках одного процессора при обработке
приема/передачи и перемещении необходимой
информации между процессом и сетевым
контроллером. Несколько запросов на
прием и/или передачу могут объединяться
вместе для того, чтобы далее их можно
было бы запустить одной командой. Способ
приема сообщения никак не зависит от
способа его посылки: сообщение,
отправленное с помощью объединения
запросов либо обычным способом, может
быть принято как обычным способом, так
и с помощью объединения запросов. 
</p>
<p>int MPI_Send_init
(void *buf, int count,
MPI_Datatype datatype, int dest, int msgtag, MPI_Comm comm,
MPI_Request *request) 
</p>
<ul>
 <li><p><em>buf</em> - адрес начала буфера посылки сообщения</p></li>
 <li><p><em>count</em> - число передаваемых элементов в сообщении</p></li>
 <li><p><em>datatype</em> - тип передаваемых элементов</p></li>
 <li><p><em>dest</em> - номер процесса-получателя</p></li>
 <li><p><em>msgtag</em> - идентификатор сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>request</em> - идентификатор асинхронной передачи</p></li>
</ul>

<p>Формирование запроса на выполнение
пересылки данных. Все параметры точно
такие же, как и у подпрограммы <em>MPI_Isend</em>,
однако в отличие от нее пересылка не
начинается до вызова подпрограммы
<em>MPI_Startall</em>. 
</p>
<p>int MPI_Recv_init
(void *buf, int count,
MPI_Datatype datatype, int source, int msgtag, MPI_Comm comm,
MPI_Request *request) 
</p>
<ul>
 <li><p>OUT <em>buf</em> - адрес начала буфера приема сообщения</p></li>
 <li><p><em>count</em> - число принимаемых элементов в сообщении</p></li>
 <li><p><em>datatype</em> - тип принимаемых элементов</p></li>
 <li><p><em>source</em> - номер процесса-отправителя</p></li>
 <li><p><em>msgtag</em> - идентификатор сообщения</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>request</em> - идентификатор асинхронного приема</p></li>
</ul>

<p>Формирование запроса на выполнение
приема данных. Все параметры точно такие
же, как и у подпрограммы <em>MPI_Irecv</em>,
однако в отличие от нее реальный прием
не начинается до вызова подпрограммы
<em>MPI_Startall</em>. 
</p>
<p>MPI_Startall (int
count, MPI_Request *requests) 
</p>
<ul>
 <li><p><em>count</em> - число запросов на взаимодействие</p></li>
 <li><p>OUT <em>requests</em> - массив идентификаторов приема/передачи</p></li>
</ul>

<p>Запуск всех отложенных взаимодействий,
ассоциированных вызовами подпрограмм
<em>MPI_Send_init</em> и <em>MPI_Recv_init</em> с элементами
массива запросов <em>requests</em>. Все
взаимодействия запускаются в режиме
без блокировки, а их завершение можно
определить обычным образом с помощью
процедур <em>MPI_Wait</em> и <em>MPI_Test</em>. 
</p>
<p>Пример
передач с отложенной блокировкой: Обмен
по кругу с ближайшими соседями.
</p>

<div class="lang-cpp"><![CDATA[
    #include <mpi.h>
    #include <stdio.h>
    int main(int argc, char **argv) {
    int numtasks, rank, next, prev, buf[2], tag1=1, tag2=2;
    MPI_Request reqs[4];
    MPI_Status stats[4];
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    prev = rank-1;
    next = rank+1;
    if(rank == 0)  prev = numtasks - 1;
    if (rank == (numtasks - 1))  next = 0;
    MPI_Irecv(&buf[0], 1, MPI_INT, prev, tag1, MPI_COMM_WORLD, &reqs[0]);
    MPI_Irecv(&buf[1], 1, MPI_INT, next, tag2, MPI_COMM_WORLD, &reqs[1]);
    MPI_Isend(&rank, 1, MPI_INT, prev, tag2, MPI_COMM_WORLD, &reqs[2]);
    MPI_Isend(&rank, 1, MPI_INT, next, tag1, MPI_COMM_WORLD, &reqs[3]);
    MPI_Waitall(4, reqs, stats);
    MPI_Finalize();
}
]]></div>


<h1 id="__RefHeading___Toc5075832">Определение размера сообщения до его помещения в приемный буфер</h1>

<p>Итак, по возвращении из MPI_Recv поля
структуры status содержат информацию о
принятом сообщении, а функция MPI_Get_count
возвращает количество фактически
принятых данных. Однако имеется еще
пара функций, которая позволяет узнать
о характеристиках сообщения <i><em>до</em></i>
того, как сообщение будет помещено в
приемный пользовательский буфер: 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><br/>
</p>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
int MPI_Probe( int source, int msgtag,
MPI_Comm comm, MPI_Status *status)

</p>
<ul>
 <li><p><em>source</em> - номер процесса-отправителя или <em>MPI_ANY_SOURCE</em></p></li>
 <li><p><em>msgtag</em> - идентификатор ожидаемого сообщения или <em>MPI_ANY_TAG</em></p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
 <li><p>OUT <em>status</em> - параметры обнаруженного сообщения</p></li>
</ul>
<p style="text-indent: 0.85cm; margin-top: 0.21cm; margin-bottom: 0cm">
Получение информации о структуре
ожидаемого сообщения с блокировкой.
Возврата из подпрограммы не произойдет
до тех пор, пока сообщение с подходящим
идентификатором и номером процесса-отправителя
не будет доступно для получения. Атрибуты
доступного сообщения можно определить
обычным образом с помощью параметра
<em>status</em>. Следует обратить внимание,
что подпрограмма определяет только
факт прихода сообщения, но реально его
не принимает. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">За
исключением адреса и размера
пользовательского буфера, она имеет
такие же параметры, как и MPI_Recv. Она
возвращает заполненную структуру
MPI_Status и после нее можно вызвать
MPI_Get_count. Стандарт MPI гарантирует, что
следующий за MPI_Probe вызов MPI_Recv <b>с теми
же</b> параметрами (имеются в виду номер
задачи-передатчика, идентификатор
сообщения и коммуникатор) поместит в
буфер пользователя именно то сообщение,
которое было принято функцией MPI_Probe.
MPI_Probe блокирует работу процесса до
момента получения требуемого сообщения.
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Аналогичная
функция <b>MPI_Iprobe</b><b>
</b>позволяет определить, пришло ли
сообщение, не блокируя процесс.</p>
<p style="text-indent: 0cm; margin-top: 0.21cm; margin-bottom: 0.18cm">
int MPI_Iprobe( int source, int msgtag,
MPI_Comm comm, int *flag, MPI_Status *status)

</p>
<p>Когда
задача-приемник не знает заранее длины
ожидаемого сообщения. Пользовательский
буфер заводится в динамической памяти:
</p>

<div class="lang-cpp"><![CDATA[
MPI_Probe(MPI_ANY_SOURCE, tagMessageInt, MPI_COMM_WORLD, &status);
/* MPI_Probe вернет управление, после того как примет  данные в системный буфер */
MPI_Get_count(&status, MPI_INT, &bufElems);
buf = malloc( sizeof(int) * bufElems );
MPI_Recv(buf, bufElems, MPI_INT, ...);
/* ... дальше параметры у MPI_Recv такие же, как в MPI_Probe,
   MPI_Recv останется просто скопировать данные
   из системного буфера в пользовательский */
]]></div>

<p>Вместо этого, конечно, можно просто завести на
приемной стороне буфер заведомо большой,
чтобы вместить в себя самое длинное из
возможных сообщений, но такой стиль не
является оптимальным, если длина
сообщений <q>гуляет</q> в слишком широких
пределах.
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">Когда
задача-приемник собирает сообщения от
разных отправителей с содержимым разных
типов. Без MPI_Probe порядок извлечения
сообщений в буфер пользователя должен
быть задан в момент компиляции: 
</p>

<div class="lang-cpp"><![CDATA[
MPI_Recv(floatBuf, floatBufSize, MPI_FLOAT, MPI_ANY_SOURCE, tagFloatData,...);
MPI_Recv(intBuf, intBufSize, MPI_INT, MPI_ANY_SOURCE, tagIntData,...);
MPI_Recv(charBuf, charBufSize, MPI_CHAR, MPI_ANY_SOURCE, tagCharData,...);
]]></div>

<p style="text-indent: 0.85cm; margin-bottom: 0cm">Теперь,
если в момент выполнения сообщение с
идентификатором tagCharData придет раньше
двух остальных, MPI будет вынужден
<q>законсервировать</q> его на время
выполнения первых двух вызовов MPI_Recv.
Это чревато непроизводительными
расходами памяти. MPI_Probe позволит задать
порядок извлечения сообщений в буфер
пользователя равным порядку их поступления
на принимающую сторону, делая это не в
момент компиляции, а непосредственно
в момент выполнения: 
</p>

<div class="lang-cpp"><![CDATA[
for (int i=0; i < 3; i++) {
    MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
    switch (status.MPI_TAG) {
        case tagFloatData:
            MPI_Recv(floatBuf, floatBufSize, MPI_FLOAT, ... );
            break;
        case tagIntData:
            MPI_Recv(intBuf, intBufSize, MPI_INT, ... );
            break;
        case tagCharData:
            MPI_Recv(charBuf, charBufSize, MPI_CHAR, ... );
            break;
    } /* конец switch */
} /* конец for */ 
]]></div>

<p>Многоточия здесь означают, что последние 4 параметра
у MPI_Recv такие же, как и у предшествующей
им MPI_Probe. 
</p>
<p><br/>
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm">
<b>Джокеры.</b> В MPI используются
два джокера: MPI_ANY_SOURCE для номера
задачи-отправителя (<q>принимай от кого угодно</q>) и MPI_ANY_TAG для идентификатора
получаемого сообщения (<q>принимай что угодно</q>). MPI резервирует для них
какие-то отрицательные целые числа, в
то время как реальные идентификаторы
задач и сообщений лежат всегда в диапазоне
от 0 до 32767. Пользоваться джокерами
следует с осторожностью, потому что по
ошибке таким вызовом MPI_Recv может быть
захвачено сообщение, которое должно
приниматься в другой части задачи-получателя.
</p>
<p>
Если логика программы достаточно сложна,
использовать джокеры можно ТОЛЬКО в
функциях MPI_Probe и <b>MPI_Iprobe</b>, чтобы перед
фактическим приемом узнать тип и
количество данных в поступившем
сообщении (на худой конец, можно принимать,
и не зная количества - был бы приемный
буфер достаточно вместительным, но тип
для MPI_Recv надо указывать <b>явно</b> - а он
может быть разным в сообщениях с
разными идентификаторами). 
</p>
<p>
Достоинство джокеров: приходящие
сообщения извлекаются по мере поступления,
а не по мере вызова MPI_Recv с нужными
идентификаторами задач/сообщений. Это
экономит память и увеличивает скорость
работы.</p>

<h1 id="__RefHeading___Toc5075833">Процедуры для коллективных коммуникаций</h1>

<p>Под
термином <q>коллективные</q> в MPI
подразумеваются три группы функций:
функции коллективного обмена данными;
точки синхронизации, или барьеры; функции
поддержки распределенных операций. 
</p>
<p>Коллективная
функция одним из аргументов получает
описатель области связи (коммуникатор).
Вызов коллективной функции является
корректным, только если произведен из
всех процессов-абонентов соответствующей
области связи, и именно с этим коммуникатором
в качестве аргумента (хотя для одной
области связи может иметься несколько
коммуникаторов, подставлять их вместо
друг друга нельзя). В этом и заключается
коллективность: либо функция вызывается
всем коллективом процессов, либо никем;
третьего не дано. 
</p>
<p>Коллективные
коммуникации затрагивают все процессы
в пределах коммуникатора. Как поступить,
если требуется ограничить область
действия для коллективной функции
только частью присоединенных к
коммуникатору задач, или наоборот -
расширить область действия? Коллективные
действия в пределах подмножества
процессов могут быть выполнены путем
разделения подмножества на новые группы,
и затем вызова коллективной операции
для коммуникатора новой группы
(обсуждается позже). Создавайте временный
коммуникатор на базе существующих, как
это показано в разделе про коммуникаторы.
</p>
<p>Коллективные
действия всегда выполняются с блокировкой.
Работают только с предопределенными
MPI типами данных – но не с производными
типами. Процедуры коллективных
коммуникаций не используют идентификаторы
сообщений пользователя. 
</p>
<p><br/>
</p>
<p>
<em><b>Точки синхронизации, они же барьеры.</b></em>
Функция <b>MPI_Barrier</b><b>
с</b>оздает барьер
синхронизации в группе.
Каждая задача, находя
вызов MPI_Barrier, блокируется,
пока все остальные процессы в группе
не дойдут до того же барьера.

</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><br/>
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><b>int
MPI_Barrier (MPI_Comm comm) </b>
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><br/>
</p>
<p><b>Это
единственная в MPI функция, вызовами
которой гарантированно синхронизируется
во времени выполнение различных ветвей!</b>
Некоторые другие коллективные функции
в зависимости от реализации могут
обладать, а могут и не обладать, свойством
одновременно возвращать управление
всем ветвям – для них это свойство
является побочным и необязательным –
если Вам нужна синхронность, используйте
<b>только</b> MPI_Barrier. 
</p>
<p>Когда может потребоваться синхронизация? 
</p>

<div class="lang-cpp"><![CDATA[
int myid, numtasks;
MPI_Comm_rank(MPI_COMM_WORLD, &myid);
MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
if (numtasks < 4) {
    if(myid == 0)  printf("Слишком мало процессов в группе\n");
    MPI_Barrier(MPI_COMM_WORLD);
    MPI_Abort(MPI_COMM_WORLD,0);
}
]]></div>

<p>В примере
синхронизация используется перед
аварийным завершением: там ветвь 0
рапортует об ошибке, и чтобы ни одна из
оставшихся ветвей вызовом MPI_Abort не
завершила нулевую досрочно-принудительно,
перед MPI_Abort поставлен барьер. 
</p>
<p>Это
утверждение не проверено, но: <i>алгоритмической</i>
необходимости в барьерах, как
представляется, нет. Параллельный
алгоритм для своего описания требует
по сравнению с алгоритмом классическим
всего лишь двух дополнительных операций
– приема и передачи из ветви в ветвь.
Точки синхронизации несут чисто
технологическую нагрузку вроде той,
что описана в предыдущем абзаце. 
</p>
<p>Иногда
случается, что ошибочно работающая
программа перестает врать, если ее
исходный текст хорошенько нашпиговать
барьерами. Как правило, барьерами
нивелируются ошибки под кодовым названием
<q><em>гонки</em></q>. Однако программа начнет
работать медленнее, например: 
</p>
<p><br/>
</p>

 <table>
  <tr>
   <td>Без барьеров</td>
   <td>
    <p>
    0
    xxxx....xxxxxxxxxxxxxxxxxxxx</p>
    <p>
    1
    xxxxxxxxxxxx....xxxxxxxxxxxx</p>
    <p>
    2
    xxxxxxxxxxxxxxxxxxxxxx....xx</p>
   </td>
  </tr>
  <tr>
   <td>С барьерами</td>
   <td>
    <p>
    0
    xxxx....xxbxxxxxxxxb||||xxxxxxxxb||xx</p>
    <p>
    1
    xxxxxxb||||x....xxxxxxxbxxxxxxxxb||xx</p>
    <p>
    2
    xxxxxxb||||xxxxxxxxb||||..xxxxxxxxbxx</p>
   </td>
  </tr>
 </table>

<p>Обозначения:
«x» – нормальное выполнение, точка –
ветвь простаивает и процессорное время
отдано под другие цели,  «b»
– вызван MPI_Barrier, «|» – MPI_Barrier ждет своего
вызова в остальных ветвях.
</p>

<p>Так что <q>задавить</q> ошибку барьерами
хорошо только в качестве временного
решения на период отладки. 
</p>
<p>
<br/>
</p>
<p>
<i><em><b>Функции коллективного обмена данными.</b></em></i> Основные особенности и
отличия от коммуникаций типа <q>точка-точка</q>: на прием и/или передачу работают одновременно <i><em>все</em></i> задачи-абоненты
указываемого коммуникатора; коллективная
функция выполняет одновременно и прием,
и передачу; она имеет большое количество
параметров, часть которых нужна для
приема, а часть для передачи; в разных
задачах та или иная часть игнорируется;
как правило, значения <i><em>всех</em></i>
параметров (за исключением адресов
буферов) должны быть <i><em>идентичными</em></i>
во всех задачах; MPI назначает идентификатор
для сообщений <i><em>автоматически</em></i>;
сообщения передаются не по указываемому
коммуникатору, а по временному
коммуникатору-<i><em>дубликату</em></i>; тем
самым потоки данных коллективных функций
надежно изолируются друг от друга и от
потоков, созданных функциями <q>точка-точка</q>.
</p>
<p>int MPI_Bcast(void *buf, int count,
MPI_Datatype datatype, int source, MPI_Comm comm)

</p>
<ul>
 <li><p>OUT <em>buf</em> - адрес начала буфера посылки сообщения</p></li>
 <li><p><em>count</em> - число передаваемых элементов в сообщении</p></li>
 <li><p><em>datatype</em> - тип передаваемых элементов</p></li>
 <li><p><em>source</em> - номер рассылающего процесса</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
</ul>
<p>Рассылка
сообщения от процесса <em>source</em> всем
процессам, включая рассылающий процесс.
При возврате из процедуры содержимое
буфера <em>buf</em> процесса <em>source</em> будет
скопировано в локальный буфер процесса.
Значения параметров <em>count</em>, <em>datatype</em>
и <em>source</em> должны быть одинаковыми у
всех процессов. 
</p>

<p>int MPI_Gather
(void *sbuf, int scount,
MPI_Datatype stype, void *rbuf, int rcount, MPI_Datatype rtype, int
dest, MPI_Comm comm) 
</p>
<ul>
 <li><p><em>sbuf</em> - адрес начала буфера посылки</p></li>
 <li><p><em>scount</em> - число элементов в посылаемом сообщении</p></li>
 <li><p><em>stype</em> - тип элементов отсылаемого сообщения</p></li>
 <li><p>OUT <em>rbuf</em> - адрес начала буфера сборки данных</p></li>
 <li><p><em>rcount</em> - число элементов в принимаемом сообщении</p></li>
 <li><p><em>rtype</em> - тип элементов принимаемого сообщения</p></li>
 <li><p><em>dest</em> - номер процесса, на котором происходит сборка данных</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
</ul>

<p><br/></p>

<p>Сборка данных со всех процессов в буфере <em>rbuf</em>
процесса <em>dest</em>. Каждый процесс, включая
<em>dest</em>, посылает содержимое своего
буфера <em>sbuf</em> процессу <em>dest</em>. Собирающий
процесс сохраняет данные в буфере <em>rbuf</em>,
располагая их в порядке возрастания
номеров процессов. Параметр <em>rbuf</em>
имеет значение только на собирающем
процессе и на остальных игнорируется,
значения параметров <em>count</em>, <em>datatype</em>
и <em>dest</em> должны быть одинаковыми у всех
процессов. 
</p>
<p>Векторный
вариант – <b>MPI_Gatherv</b> – позволяет задавать
<i><em>разное</em></i> количество отправляемых
данных в разных задачах-отправителях.
Соответственно, на приемной стороне
задается массив позиций в приемном
буфере, по которым следует размещать
поступающие данные, и максимальные
длины порций данных от всех задач. Оба
массива содержат позиции/длины не в
байтах, а в количестве ячеек типа
recvCount. 
</p>
<p>int MPI_Scatter
(void *sbuf, int scount,
MPI_Datatype stype, void *rbuf, int rcount, MPI_Datatype rtype, int
root, MPI_Comm comm) 
</p>
<p><q>Разбрызгиватель</q>: выполняет обратную <b>MPI_Gather</b>
операцию – части передающего буфера
из задачи root распределяются по приемным
буферам всех задач. 
</p>
<p>И
векторный вариант: <b>MPI_Scatterv</b>, рассылающий
части неодинаковой длины в приемные
буферы неодинаковой длины. 
</p>

<p>int MPI_Allgather
(void *sbuf, int scount,
MPI_Datatype stype, void *rbuf, int rcount, MPI_Datatype rtype,
MPI_Comm comm) 
</p>
<p>Аналогична
MPI_Gather, но прием осуществляется не в
одной задаче, а во <i><em>всех</em></i>: каждая
имеет специфическое содержимое в
передающем буфере, и все получают
одинаковое содержимое в буфере приемном.
Как и в MPI_Gather, приемный буфер последовательно
заполняется данными изо всех передающих.
Вариант с неодинаковым количеством
данных называется <b>MPI_Allgatherv</b>. 
</p>

<p>int MPI_Alltoall
(void *sbuf, int scount,
MPI_Datatype stype, void *rbuf, int rcount, MPI_Datatype rtype,
MPI_Comm comm) 
</p>
<p>Каждый
процесс нарезает передающий буфер на
куски и рассылает куски остальным
процессам; каждый процесс получает
куски от всех остальных и поочередно
размещает их приемном буфере. Это Scatter
и Gather в одном флаконе.
Векторный вариант называется <b>MPI_Alltoallv</b>.
</p>
<p>Помните,
что коллективные функции несовместимы с <q>точка-точка</q> передачами:
недопустимым, например, является вызов
в одной из принимающих широковещательное
сообщение задач MPI_Recv вместо MPI_Bcast. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><br/>
</p>
<p><i><em><b>Распределенные
операции.</b></em></i> Идея проста: в каждой
задаче имеется массив. Над нулевыми
ячейками всех массивов производится
некоторая операция (сложение/произведение/
поиск минимума/максимума и т.д.), над
первыми ячейками производится такая
же операция и т.д. Четыре функции
предназначены для вызова этих операций
и отличаются способом размещения
результата в задачах. 
</p>
<p>Предопределенных
описателей операций в MPI насчитывается
12. MPI_MAX и MPI_MIN ищут поэлементные максимум
и минимум. MPI_SUM вычисляет сумму векторов.
MPI_PROD вычисляет поэлементное произведение
векторов. MPI_LAND, MPI_BAND, MPI_LOR, MPI_BOR, MPI_LXOR,
MPI_BXOR – логические и двоичные операции
«И», «ИЛИ», «исключающее ИЛИ». MPI_MAXLOC,
MPI_MINLOC – поиск индексированного
минимума/максимума - здесь не
рассматриваются.</p>
<p>Естественный
вопрос – с массивами каких типов умеют
работать эти функции? Ответ приводится
в виде таблицы. Количество типов для
ячеек векторов, поддерживающих ту или
иную операцию, строго ограничено ниже
перечисленными. Никакие другие встроенные
или пользовательские описатели типов
использоваться не могут! Обратите также
внимание, что все операции являются
ассоциативными (<q>(a+b)+c = a+(b+c)</q>) и
коммутативными (<q>a+b = b+a</q>). Помимо
встроенных операций, пользователь может
вводить свои собственные операции. Для
этого служат функции MPI_Op_create и MPI_Op_free,
а также тип MPI_User_function.
</p>


<table>
<tr>
<th colspan="2">MPI Reduction Operation</th>
<th>C data types</th>
<th>Fortran data types</th>
</tr>
<tr>
   <td>MPI_MAX</td>
   <td>maximum</td>
   <td>integer, float</td>
   <td>integer, real, complex</td>
</tr>
<tr>
   <td>
    
    MPI_MIN
   </td>
   <td>
    minimum
   </td>
   <td>
    integer,
    float
   </td>
   <td>
    integer,
    real, complex
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_SUM
   </td>
   <td>
    sum
   </td>
   <td>
    integer,
    float
   </td>
   <td>
    integer,
    real, complex
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_PROD
   </td>
   <td>
    product
   </td>
   <td>
    integer,
    float
   </td>
   <td>
    integer,
    real, complex
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_LAND
   </td>
   <td>
    logical
    AND
   </td>
   <td>
    integer
   </td>
   <td>
    logical
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_BAND
   </td>
   <td>
    bit-wise
    AND
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_LOR
   </td>
   <td>
    logical
    OR
   </td>
   <td>
    integer
   </td>
   <td>
    logical
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_BOR
   </td>
   <td>
    bit-wise
    OR
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_LXOR
   </td>
   <td>
    logical
    XOR
   </td>
   <td>
    integer
   </td>
   <td>
    logical
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_BXOR
   </td>
   <td>
    bit-wise
    XOR
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
   <td>
    integer,
    MPI_BYTE
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_MAXLOC
   </td>
   <td>
    max
    value and location
   </td>
   <td>
    float,
    double and long double
   </td>
   <td>
    
    real, complex,double precision
   </td>
  </tr>
  <tr>
   <td>
    
    MPI_MINLOC
   </td>
   <td>
    min
    value and location
   </td>
   <td>
    float,
    double and long double
   </td>
   <td>
    real,
    complex, double precision
   </td>
  </tr>
 </table>

<p>int MPI_Allreduce( void *sbuf, void *rbuf,
int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) 
</p>

<ul>
 <li><p><em>sbuf</em> - адрес начала буфера для аргументов</p></li>
 <li><p>OUT <em>rbuf</em> - адрес начала буфера для результата</p></li>
 <li><p><em>count</em> - число аргументов у каждого процесса</p></li>
 <li><p><em>datatype</em> - тип аргументов</p></li>
 <li><p><em>op</em> - идентификатор глобальной операции</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
</ul>
<p>Выполнение <em>count</em> глобальных операций
<em>op</em> с возвратом <em>count</em> результатов
во всех процессах в буфере <em>rbuf</em>.
Операция выполняется независимо над
соответствующими аргументами всех
процессов. Значения параметров <em>count</em>
и <em>datatype</em> у всех процессов должны быть
одинаковыми. 
</p>
<p>int MPI_Reduce( void *sbuf, void *rbuf,
int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)

</p>
<ul>
 <li><p><em>sbuf</em> - адрес начала буфера для аргументов</p></li>
 <li><p>OUT <em>rbuf</em> - адрес начала буфера для результата</p></li>
 <li><p><em>count</em> - число аргументов у каждого процесса</p></li>
 <li><p><em>datatype</em> - тип аргументов</p></li>
 <li><p><em>op</em> - идентификатор глобальной операции</p></li>
 <li><p><em>root</em> - процесс-получатель результата</p></li>
 <li><p><em>comm</em> - идентификатор группы</p></li>
</ul>

<p>Функция аналогична предыдущей, но
результат будет записан в буфер <em>rbuf</em>
только у процесса <em>root</em>. 
</p>
<p>int MPI_Reduce_scatter( void *sbuf, void
*rbuf, int *recvcount, MPI_Datatype datatype, MPI_Op op, MPI_Comm
comm) 
</p>
<p>
Сначала делается векторная
распределенная операция
для всех процессов в группе.
Затем, вектор результата
делится на части и
распределяется между
процессами. Это эквивалентно
отдельным операциям
MPI_Reduce и MPI_Scatter.
Каждая задача получает не весь
массив-результат, а его часть. Длины
этих частей находятся в массиве –
третьем параметре функции. Размер
исходных массивов во всех задачах
одинаков и равен сумме длин результирующих
массивов.</p>
<p>
int MPI_Scan (void *sbuf, void *rbuf, int
count, MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) 
</p>
<p>Аналогична функции MPI_Allreduce в том отношении, что
каждая задача получает результирующий
массив. Главное отличие: здесь содержимое
массива-результата в задаче <b>i</b> является
результатом выполнение операции над
массивами из задач с номерами от <b>0</b>
до <b>i</b> включительно. 
</p>
<p>Пример коллективных коммуникаций: Раздача строк матрицы.</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define SIZE 4
int main(int argc, char **argv) {
    int numtasks, rank, sendcount, recvcount, source;
    float sendbuf[SIZE][SIZE] = {
     {1.0,  2.0, 3.0, 4.0},
     {5.0,  6.0, 7.0, 8.0},
     {9.0, 10.0, 11.0, 12.0},
     {13.0, 14.0, 15.0, 16.0}  };
    float recvbuf[SIZE];
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    if (numtasks == SIZE) {
     source = 1;
     sendcount = recvcount = SIZE;
     MPI_Scatter(sendbuf,sendcount,MPI_FLOAT,recvbuf,recvcount,
                MPI_FLOAT, source, MPI_COMM_WORLD);
     printf("rank=%d  Results: %f %f %f %f\n", rank,recvbuf[0],
            recvbuf[1],recvbuf[2],recvbuf[3]);
     }else
     printf("Must specify %d processors. Terminating.\n", SIZE);
    MPI_Finalize();
}
]]></div>

<p>Пример программного вывода: 
</p>
<p><b>rank=0  Results: 1.000000 2.000000 3.000000 4.000000</b></p>
<p><b>rank=1  Results: 5.000000 6.000000 7.000000 8.000000</b></p>
<p><b>rank=2  Results: 9.000000 10.000000 11.000000 12.000000</b></p>
<p><b>rank=3  Results: 13.000000 14.000000 15.000000 16.000000</b></p>

<h1 id="__RefHeading___Toc5075834">Зачем MPI знать тип передаваемых данных?</h1>

<p>Действительно, зачем? Стандартные функции пересылки
данных, например, <dfn>memcpy</dfn>, прекрасно
обходятся без подобной информации –
им требуется знать только размер
в байтах. Вместо одного такого
аргумента функции MPI получают два:
количество элементов некоторого типа
и символический описатель указанного
типа (MPI_INT, и т.д.). Причин тому несколько:
</p>
<p>Пользователю
MPI позволяет описывать свои собственные
типы данных, которые располагаются
в памяти не непрерывно, а с разрывами,
или, наоборот, с <q>налезаниями</q> друг
на друга. Переменная такого типа
характеризуется не только размером, и
эти характеристики MPI хранит в описателе
типа. В учебнике по MPI приведен пример
конструирования сложного типового
шаблона путем последовательного создания
нескольких пользовательских типов.
Затем производится транспонирование
матрицы через единственный вызов
MPI_Sendrecv, где для передачи в качестве
описателя используется указанный
шаблон. 
</p>
<p>Приложение
MPI может работать на гетерогенном
вычислительном комплексе (коллективе
ЭВМ с разной архитектурой). Одни и те же
типы данных на разных машинах могут
иметь разное представление, например:
на плавающую арифметику существует 3 разных стандарта (IEEE, IBM, Cray);
тип char в терминальных приложениях Windows представлен альтернативной
кодировкой ГОСТ, а в Юниксе – кодировкой KOI-8r;
ориентация байтов в многобайтовых числах на ЭВМ с процессорами Intel отличается от общепринятой (у Intel –
младший байт занимает младший адрес, у всех остальных – наоборот).
Если приложение работает в гетерогенной сети, через сеть задачи обмениваются
данными в формате XDR (eXternal Data Representation), принятом в Internet.
Перед отправкой и после приема данных задача конвертирует их в/из формата XDR.
Естественно, при этом MPI должен знать не просто количество передаваемых байтов, но и тип содержимого.
</p>

<p>Такие часто используемые в Си типы данных,
как структуры, могут содержать в себе
некоторое пустое пространство, чтобы
все поля в переменной такого типа
размещались по адресам, кратным некоторому
четному числу (часто 2, 4 или 8) - это
ускоряет обращение к ним. Причины тому
чисто аппаратные. Выравнивание данных
настраивается ключами компилятора.
Разные задачи одного и того же приложения,
выполняющиеся на одной и той же машине
(даже на одном и том же процессоре), могут
быть построены с разным выравниванием,
и типы с одинаковым текстовым описанием
будут иметь разное двоичное представление.
MPI будет вынужден позаботиться о
правильном преобразовании. Например,
переменные такого типа могут занимать
9, 10, 12 или 16 байт:
</p>

<div class="lang-cpp"><![CDATA[
typedef struct {
    char c;
    double  d;
} CharDouble;
]]></div>

<p>Создание собственных типов описывается дальше по тексту.
</p>

<p><i><em><b>Неэффективная передача разнотипных данных.</b></em></i> Все
функции приема-передачи оперируют
массивами - непрерывными последовательностями
однотипных данных. Однако программисту
может потребоваться пересылать данные,
которые либо разнотипны, либо не-непрерывно
расположены в памяти, либо то и другое
сразу (особо тяжелый случай). 
</p>
<p><em>Вариант 1</em> (настолько тупой и медленный, что
никогда не применяется). Каждый элемент
в разнотипном наборе данных посылается
отдельно: 
</p>

<div class="lang-cpp"><![CDATA[
#define msgTag 10

struct {
    int i;
    float f[4];
    char  c[8];
} s;

MPI_Send(&s.i, 1, MPI_INT, targetRank, msgTag,  comm );
MPI_Send(s.f, 4, MPI_FLOAT, targetRank, msgTag+1, comm );
MPI_Send(s.c, 8, MPI_CHAR,  targetRank, msgTag+2, comm );
// ... и на приемной стороне столько же раз вызывается MPI_Recv.
]]></div>

<p><em>Вариант 2</em> (<q>классический</q>). Функция
приема/передачи вызывается один раз,
но до/после нее многократно вызывается
функция упаковки/распаковки: 
</p>

<div class="lang-cpp"><![CDATA[
// Передача:
int bufPos = 0;
char tempBuf [sizeof(s)];

MPI_Pack(&s.i, 1, MPI_INT,tempBuf,sizeof(tempBuf),&bufPos,comm);
MPI_Pack(s.f,4,MPI_FLOAT,tempBuf,sizeof(tempBuf),&bufPos,comm);
MPI_Pack(s.c,8,MPI_CHAR,tempBuf,sizeof(tempBuf),&bufPos,comm );
MPI_Send(tempBuf,bufPos,MPI_BYTE,targetRank,msgTag,comm);
// Прием:

int bufPos = 0;
char tempBuf[sizeof(s)];

MPI_Recv(tempBuf,sizeof(tempBuf),MPI_BYTE,sourceRank,msgTag,comm,&status);
MPI_Unpack(tempBuf,sizeof(tempBuf),&bufPos,&s.i,1,MPI_INT,comm);
MPI_Unpack(tempBuf,sizeof(tempBuf),&bufPos,s.f,4,MPI_FLOAT,comm);
MPI_Unpack(tempBuf,sizeof(tempBuf),&bufPos,s.c,8,MPI_CHAR,comm);
]]></div>

<p>Вариант 2 обозван здесь классическим, потому
что пришел в MPI из PVM, где предлагается
в качестве единственного. Он прост в
понимании, за что его все и любят.
Замечания по применению: 
</p>
<p>
<b>MPI_BYTE</b> – особый описатель типа, который
не описывает тип данных для конкретного
языка программирования (в Си он ближе
всего к <b>unsigned char</b>). Использование
MPI_BYTE означает, что содержимое
соответствующего массива <i><em>не должно</em></i>
подвергаться <i><em>никаким</em></i> преобразованиям
– и на приемной, и на передающей стороне
массив будет иметь одну и ту же длину и
одинаковое <i><em>двоичное</em></i> представление.
</p>
<p>Зачем
функциям упаковки/распаковки требуется
описатель области связи? Описатель,
помимо прочего, несет в себе информацию
о распределении подсоединенных к области
связи задач по процессорам и компьютерам.
Если процессоры одинаковые, или задачи
выполняются на одном и том же процессоре,
данные просто копируются, иначе происходит
их преобразование в промежуточный
формат <b>XDR</b> (eXternal Data Representation – разработан
фирмой Sun Microsystems, используется в Интернете
для взаимодействия разнотипных машин).
Коммуникаторы у функций упаковки/распаковки
и у соответствующей функции передачи/приема
должны совпадать, иначе произойдет
ошибка. 
</p>
<p>По мере
того как во временный буфер помещаются
данные или извлекаются оттуда, MPI
сохраняет текущую позицию в переменной,
которая в приведенном примере названа
<b>bufPos</b>. Не забудьте проинициализировать
ее нулем, перед тем как начинать
упаковывать/извлекать. Естественно,
что передается она не по значению, а по
ссылке. Первый же аргумент – <q>адрес временного буфера</q> – во всех вызовах
остается неизменным. 
</p>
<p>В примере
<i><em>некорректно</em></i> выбран размер
временного буфера: использовалось
<i><em>неверное</em></i> предположение, что в
XDR-формате данные займут места не больше,
чем в формате используемого ветвью
процессора; или что XDR-преобразование
заведомо не будет применено. Правильным
же решением будет для определения
необходимого размера временного буфера
на приемной стороне использовать связку
MPI_Probe / MPI_Get_count / MPI_Recv, а на передающей -
функцию MPI_Pack_size: 
</p>

<div class="lang-cpp"><![CDATA[
int bufSize = 0;
void *tempBuf;
MPI_Pack_size(1, MPI_INT,   MPI_COMM_WORLD, &bufSize);
MPI_Pack_size(4, MPI_FLOAT, MPI_COMM_WORLD, &bufSize);
MPI_Pack_size(8, MPI_CHAR,  MPI_COMM_WORLD, &bufSize);
tempBuf = malloc(bufSize);
/* ... теперь можем упаковывать, не опасаясь переполнения */
]]></div>

<p>Однако и вариант 2 замедляет работу:
по сравнению с единственным вызовом <b>memcpy</b> на SMP-машине или одном процессоре,
нудная упаковка/распаковка – дело весьма небыстрое!
</p>

<p><em>Вариант 3</em> (<q>жульнический</q>). Если есть
уверенность, что одни и те же типы данных
в обеих ветвях приложения имеют одинаковое
двоичное представление, то:
</p>

<div class="lang-cpp"><![CDATA[
// Передача:
MPI_Send (&s, sizeof(s), MPI_BYTE...);
// Прием:
MPI_Recv(&s, sizeof(s), MPI_BYTE...); 
]]></div>

<p>А все, чем чреват такой подход, подробно перечислялось ранее. 
</p>

<h1 id="__RefHeading___Toc5075835">Создание и использование собственных типов данных.</h1>

<p>MPI также обеспечивает программиста средствами для определения собственных
структур данных, основанных на комбинировании примитивных типов данных MPI.
Такие определенные пользователем структуры называются производными типами данных.
Пользовательские описатели типов создаются на базе созданных ранее
пользовательских описателей, и на базе встроенных описателей;
встроенные описатели имеются для <i><em>всех</em></i>
стандартных типов: MPI_INT, MPI_CHAR, MPI_LONG,
MPI_FLOAT, MPI_DOUBLE и так далее; тип MPI_BYTE служит
для передачи двоичных данных; после
конструирования, но перед использованием
описатель должен быть зарегистрирован
функцией MPI_Type_commit; после использования
описатель должен быть удален функцией
MPI_Type_free. 
</p>
<p>
Примитивные типы данных занимают в
памяти смежную область. Производные
типы позволяют в удобной манере определять
данные, состоящие из нескольких несмежных
участков, и обращаться с ними, как если
бы они были смежны. MPI обеспечивает
несколько методов для строительства
полученных типов данных: массив, вектор,
индексированный вектор, структуры. 
</p>
<p>
<br/>
</p>
<p>
<b>MPI_Type_commit.</b> Передает системе новый
тип данных. Требуется для создания всех
пользовательских типов. 
</p>
<p>
<b>MPI_Type_commit
(datatype)</b></p>
<p>
<b>MPI_TYPE_COMMIT
(datatype,ierr) </b>
</p>
<p>
<b>MPI_Type_contiguous</b><b>:</b>
самый простой конструктор типа, он
создает описание массива. 
</p>
<p>
<b>MPI_Type_contiguous
(count,oldtype,*newtype) </b>
</p>
<p>
<b>MPI_TYPE_CONTIGUOUS
(count,oldtype,newtype,ierr) </b>
</p>
<p>
<br/>
</p>
<p>
Функция <b>MPI_Type_count</b> вернет количество
ячеек в переменной составного типа:
после MPI_Type_count(intArray16, &amp;count) значение
count станет равным 16. Как правило, нет
прямой необходимости использовать эту
функцию. 
</p>
<p style="text-indent: 0.85cm; margin-bottom: 0cm"><br/>
</p>
<p>Пример: Создание типа данных, представляющего строку матрицы и рассылка разных строк всем процессам.</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define  SIZE 4
int main(int argc, char **argv) {
    int numtasks, rank, source=0, dest, tag=1, i;
    float a[SIZE][SIZE] =
     {1.0, 2.0, 3.0, 4.0,
      5.0, 6.0, 7.0, 8.0,
      9.0, 10.0, 11.0, 12.0,
      13.0, 14.0, 15.0, 16.0};
    float  b[SIZE];
    MPI_Status stat;
    MPI_Datatype  rowtype;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    MPI_Type_contiguous(SIZE, MPI_FLOAT, &rowtype);
    MPI_Type_commit(&rowtype);
    if(numtasks == SIZE) {
     if(rank == 0) {
        for (i=0; i < numtasks; i++)
          MPI_Send(&a[i][0], 1, rowtype, i, tag, MPI_COMM_WORLD);
     }
     MPI_Recv(b,SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &stat);
     printf("rank=%d  b= %3.1f %3.1f %3.1f %3.1f\n",
            rank,b[0],b[1],b[2],b[3]);
    }
    else
     printf("Must specify %d processors. Terminating.\n", SIZE);
    MPI_Finalize();
}
]]></div>


<p>Пример программного вывода: 
</p>
<p><b>rank=0  b= 1.0 2.0 3.0 4.0</b></p>
<p><b>rank=1  b= 5.0 6.0 7.0 8.0</b></p>
<p><b>rank=2  b= 9.0 10.0 11.0 12.0</b></p>
<p><b>rank=3  b= 13.0 14.0 15.0 16.0</b></p>


<p><b>MPI_Type_vector</b>:
служит для описания множества однотипных
равноудаленных в памяти массивов данных.
Позволяет весьма изощренные манипуляции
с данными. Он создает описание для
не-непрерывной последовательности
элементов, которые, в свою очередь,
составлены из непрерывной последовательности
ячеек базового (уже определенного) типа:
</p>
<p>MPI_Type_vector
(int count, /* количество элементов в новом типе */</p>
<p style="margin-left: 1.91cm; margin-bottom: 0cm">  int
blocklength, /* количество ячеек базового типа в одном элементе */</p>
<p style="margin-left: 1.91cm; margin-bottom: 0cm">  int
stride,      /* расстояние между <i><em>началами</em></i>
эл-тов, в числе ячеек */</p>
<p style="margin-left: 1.91cm; margin-bottom: 0cm"> 
MPI_Datatype  oldtype,  /* описатель базового типа, т.е.
типа ячейки */</p>
<p style="margin-left: 1.91cm; margin-bottom: 0cm"> 
MPI_Datatype  &amp;newtype);
 /* cсылка на новый описатель */</p>
<p>
То есть: новый тип состоит из элементов;
каждый элемент является массивом ячеек
базового типа; расстояние в количестве
ячеек задается между началами элементов,
а не между <i><em>концом</em></i> предыдущего
и <i><em>началом</em></i> следующего; таким
образом, элементы могут и располагаться
с разрывами, и <q>налезать</q> друг на
друга. 
</p>
<p>
Функция <b>MPI_Type_hvector</b> полностью ей
аналогична, за одним исключением:
расстояние между элементами задается
не в количестве ячеек базового типа, а
в байтах.</p>
<p>Пример:
Создание типа данных для
столбцов матрицы и
распределение информации
из разных столбцов всем процессам.</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define SIZE 4
int main(int argc, char **argv) {
    int numtasks, rank, source=0, dest, tag=1, i;
    float a[SIZE][SIZE] = 
     {1.0, 2.0, 3.0, 4.0,  
      5.0, 6.0, 7.0, 8.0, 
      9.0, 10.0, 11.0, 12.0,
     13.0, 14.0, 15.0, 16.0};
    float b[SIZE]; 
    MPI_Status stat;
    MPI_Datatype columntype;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    MPI_Type_vector(SIZE, 1, SIZE, MPI_FLOAT, &columntype);
    MPI_Type_commit(&columntype);
    if (numtasks == SIZE) {
     if (rank == 0) {
        for (i=0; i < numtasks; i++) 
          MPI_Send(&a[0][i], 1, columntype, i, tag, MPI_COMM_WORLD);
      }
     MPI_Recv(b, SIZE, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &stat);
     printf("rank=%d  b= %3.1f %3.1f %3.1f %3.1f\n",
           rank,b[0],b[1],b[2],b[3]);
     }
    else
     printf("Must specify %d processors. Terminating.\n", SIZE);
    MPI_Finalize();
}
]]></div>

<p>Пример программного вывода: 
</p>
<p><b>rank=0  b= 1.0 5.0 9.0 13.0</b></p>
<p><b>rank=1  b= 2.0 6.0 10.0 14.0</b></p>
<p><b>rank=2  b= 3.0 7.0 11.0 15.0</b></p>
<p><b>rank=3  b= 4.0 8.0 12.0 16.0</b></p>

<p><b>MPI_Type_indexed</b>:
расширение <q>векторного</q> описателя;
длины массивов и расстояния между ними
теперь не фиксированы, а у каждого
массива свои. Соответственно, аргументы
№2 и №3 здесь – не переменные, а массивы:
массив длин и массив позиций. 
</p>
<p>Пример:
создание шаблона для выделения правой
верхней части матрицы. 
</p>

<div class="lang-cpp"><![CDATA[
/* кол-во массивов в переменной нового типа */
#define SIZE  100
float a[SIZE][SIZE];
int len[SIZE];  /* длины этих массивов */
int pos[SIZE]; /*их позиции от начала, отсчитываемые в количестве ячеек */
MPI_Datatype upper;
...
for (i=0; i < SIZE; i++ ) {
    pos[i] = SIZE*i + i;
    len[i] = SIZE - i;
}
MPI_Type_indexed(SIZE, len,pos, MPI_FLOAT, &upper);
MPI_Type_commit(&upper);
/* Поступающий поток чисел типа 'float' будет размещен в верхней правой части матрицы 'a'  */
MPI_Recv(a, 1, upper, ....);
]]></div>

<p>
Аналогично работает функция
<b>MPI_Type_hindexed</b>, но позиции массивов от
начала переменной задаются не в количестве
ячеек базового типа, а в байтах. 
</p>
<p>Пример:
Создание типа данных для выделения
переменной длины порций массива
и передачи их всем процессам.</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define NELEMENTS 6
int main(int argc, char **argv) {
    int numtasks, rank, source=0, dest, tag=1, i;
    int blocklengths[2], displacements[2];
    float a[16] = {
      1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 
      9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0};
    float b[NELEMENTS]; 
    MPI_Status stat;
    MPI_Datatype indextype;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    blocklengths[0] = 4;
    blocklengths[1] = 2;
    displacements[0] = 5;
    displacements[1] = 12;
    MPI_Type_indexed(2, blocklengths, displacements, MPI_FLOAT, &indextype);
    MPI_Type_commit(&indextype);
    if(rank == 0) {
     for(i=0; i < numtasks; i++) 
        MPI_Send(a, 1, indextype, i, tag, MPI_COMM_WORLD);
     }
    MPI_Recv(b, NELEMENTS, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &stat);
    printf("rank=%d  b= %3.1f %3.1f %3.1f %3.1f %3.1f %3.1f\n",
        rank,b[0],b[1],b[2],b[3],b[4],b[5]);
    MPI_Finalize();
}
]]></div>


<p>Пример программного вывода: 
</p>
<p><b>rank=0  b= 6.0 7.0 8.0 9.0 13.0 14.0</b></p>
<p><b>rank=1  b= 6.0 7.0 8.0 9.0 13.0 14.0</b></p>
<p><b>rank=2  b= 6.0 7.0 8.0 9.0 13.0 14.0</b></p>
<p><b>rank=3  b= 6.0 7.0 8.0 9.0 13.0 14.0</b></p>

<p><b>MPI_Type_struct</b>:
создает описатель структуры. Наверняка
будет использоваться Вами чаще всего.
</p>
<p style="margin-left: 1.59cm; text-indent: 0.64cm; margin-bottom: 0cm">
MPI_Type_struct(count,
              /* количество полей */</p>
<p style="margin-left: 1.59cm; text-indent: 0.64cm; margin-bottom: 0cm">
    int
*len,   /* массив с длинами полей  (на тот
случай, если это массивы) */</p>
<p style="margin-left: 1.59cm; text-indent: 0.64cm; margin-bottom: 0cm">
    MPI_Aint
*pos,       /* массив со смещениями полей  от
начала структуры, в байтах */</p>
<p style="margin-left: 1.59cm; text-indent: 0.64cm; margin-bottom: 0cm">
    MPI_Datatype
*types, /* массив с описателями типов полей
*/</p>
<p style="margin-left: 1.59cm; text-indent: 0.64cm; margin-bottom: 0cm">
    MPI_Datatype
*newtype ); /* ссылка на создаваемый тип */ 
</p>
<p>Здесь
используется тип <b>MPI_Aint</b>: это просто
скалярный тип, переменная которого
имеет одинаковый с указателем размер.
Введен он исключительно для единообразия
с Фортраном, в котором нет типа <q>указатель</q>.
По этой же причине имеется и функция <b>MPI_Address</b>: в Си она не нужна (используются
оператор вычисления адреса &amp; и
основанный на нем макрос offsetof()); а в
Фортране оператора вычисления адреса
нет, и используется <b>MPI_Address</b>. 
</p>
<p>Пример
создания описателя типа <q>структура</q>:
</p>

<div class="lang-cpp"><![CDATA[
#include <stddef.h>  /* подключаем макрос 'offsetof()' */

typedef struct {
    int    i;
    double d[3];
    long   l[8];
    char   c;
} AnyStruct;

AnyStruct st;
MPI_Datatype anyStructType;

int len[5] = { 1, 3, 8, 1, 1 };

MPI_Aint pos[5] = {
    offsetof(AnyStruct,i),
    offsetof(AnyStruct,d),
    offsetof(AnyStruct,l),
    offsetof(AnyStruct,c),
    sizeof(AnyStruct)
};

MPI_Datatype typ[5] = {
    MPI_INT,
    MPI_DOUBLE,
    MPI_LONG,
    MPI_CHAR,
    MPI_UB
};

MPI_Type_struct(5, len, pos, typ, &anyStructType );
MPI_Type_commit(&anyStructType);  /* подготовка закончена */
MPI_Send(st, 1, anyStructType, ...);
]]></div>

<p>Обратите внимание: структура в примере содержит
4 поля, а массивы для ее описания состоят
из 5 элементов. Сделано это потому, что
MPI должен знать не только смещения полей,
но и размер всей структуры. Для этого и
служит псевдотип <b>MPI_UB</b> (<q>upper bound</q>). <br/>Адрес начала структуры и
адрес ее первого поля, как правило,
совпадают, но если это не так: нулевым
элементом массива typ должен быть MPI_LB.</p>

<p>Пример: Создание типа данных, представляющего частицу и пересылка массива частиц всем процессам.</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define NELEM 25

typedef struct {
   float x, y, z;
   float velocity;
   int   n;
   int   type;
} Particle;

int main(int argc, char **argv) {
    int numtasks, rank, source=0, dest, tag=1, i;
    Particle p[NELEM], particles[NELEM];
    MPI_Datatype particletype, oldtypes[2]; 
    int blockcounts[2];
    /* MPI_Aint type used to be consistent with syntax of */
    /* MPI_Type_extent routine */
    MPI_Aint offsets[2], extent;
    MPI_Status stat;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    /* Setup description of the 4 MPI_FLOAT fields x, y, z, velocity */
    offsets[0] = 0;
    oldtypes[0] = MPI_FLOAT;
    blockcounts[0] = 4;
    /* Setup description of the 2 MPI_INT fields n, type */
    /* Need to first figure offset by getting size of MPI_FLOAT */
    MPI_Type_extent(MPI_FLOAT, &extent);
    offsets[1] = 4 * extent;
    oldtypes[1] = MPI_INT;
    blockcounts[1] = 2;
    /* Now define structured type and commit it */
    MPI_Type_struct(2, blockcounts, offsets, oldtypes, &particletype);
    MPI_Type_commit(&particletype);
    /* Initialize the particle array and then send it to each task */
    if(rank == 0) {
        for (i = 0; i < NELEM; i++) {
           particles[i].x = i * 1.0;
           particles[i].y = i * -1.0;
           particles[i].z = i * 1.0; 
           particles[i].velocity = 0.25;
           particles[i].n = i;
           particles[i].type = i % 2; 
        }
        for (i = 0; i < numtasks; i++)
           MPI_Send(particles, NELEM, particletype, i, tag, MPI_COMM_WORLD);
    }
    MPI_Recv(p, NELEM, particletype, source, tag, MPI_COMM_WORLD, &stat);
    /* Print a sample of what was received */
    printf("rank=%d   %3.2f %3.2f %3.2f %3.2f %d %d\n", rank,p[3].x,
       p[3].y,p[3].z,p[3].velocity,p[3].n,p[3].type);
    MPI_Finalize();
    return 0;
}
]]></div>


<p>Пример программного вывода: 
</p>
<p><b>rank=0   3.00 -3.00 3.00 0.25 3 1</b></p>
<p><b>rank=2   3.00 -3.00 3.00 0.25 3 1</b></p>
<p><b>rank=1   3.00 -3.00 3.00 0.25 3 1</b></p>
<p><b>rank=3   3.00 -3.00 3.00 0.25 3 1</b></p>

<p><b>MPI_Type_extent</b> и <b>MPI_Type_size</b>: важные информационные
функции. Их характеристики удобно представить в виде таблицы: 
</p>

<table>
<thead>
<tr>
   <th>Вид данных</th>
   <th>sizeof</th>
   <th>MPI_Type_extent</th>
   <th>MPI_type_size</th>
</tr>
</thead>
<tbody>
<tr>
   <td>стандартный тип</td>
   <td colspan="3">равносильны</td>
</tr>
<tr>
   <td>массив</td>
   <td colspan="3">равносильны</td>
</tr>
<tr>
   <td>структура</td>
   <td colspan="2">равносильны</td>
   <td>sizeof(поле1)+sizeof(поле2)+...</td>
 </tr>
 <tr>
   <td>Описатель типа MPI с перекрытиями и разрывами</td>
   <td>не определена</td>
   <td>адрес последней ячейки данных - <br/>адрес первой ячейки данных + <br/>sizeof(последней ячейки данных)</td>
   <td>sizeof(первой ячейки данных) + <br/>sizeof(второй ячейки данных) + ...</td>
</tr>
</tbody>
</table>

<p>Можно сказать, что MPI_Type_extent сообщает, сколько
места переменная типа занимает при
хранении в памяти, а MPI_Type_size – какой
<i><em>минимальный </em></i>размер она будет
иметь при передаче (сжатая за счет
неиспользуемого пространства).
В Фортране их придется использовать постоянно ввиду отсутствия <b>sizeof</b>. 
</p>

<h1 id="__RefHeading___Toc5075836">Коммуникаторы, группы и области связи.</h1>

<p><em>Группа</em>
– это упорядоченное
множество процессов. Один процесс может
быть членом нескольких групп. В
MPI группа представлена
как объект в системной памяти.
В распоряжение программиста предоставлен
тип <b>MPI_Group</b> (дескриптор системного
объекта) и набор функций, работающих с
переменными и константами этого типа.
Констант, собственно, две: MPI_GROUP_EMPTY может
быть возвращена, если группа с
запрашиваемыми характеристиками в
принципе может быть создана, но пока не
содержит ни одной ветви; MPI_GROUP_NULL
возвращается, когда запрашиваемые
характеристики противоречивы. Согласно
концепции MPI, после создания группу
нельзя дополнить или усечь – можно
создать только новую группу под требуемый
набор процессов на базе существующей.
</p>
<p><em>Область связи</em> (<q>communication domain</q>) - это нечто абстрактное:
в распоряжении программиста нет типа данных, описывающего непосредственно области связи, как нет и функций по управлению ими.
Области связи автоматически создаются и уничтожаются вместе с коммуникаторами.
Абонентами одной области связи являются <i><em>все</em></i> задачи либо одной, либо двух групп.
</p>
<p><em>Коммуникатор</em>
или описатель области связи – это
верхушка трехслойного пирога: группы,
области связи, описатели областей связи.
Коммуникатор – это группа процессов,
между которыми допустимы коллективные
коммуникации. Все сообщения
MPI относятся к определенному
коммуникатору. Именно с
коммуникаторами программист имеет
дело, вызывая функции пересылки данных,
а также подавляющую часть вспомогательных
функций. Одной области связи могут
соответствовать несколько коммуникаторов.
Коммуникаторы являются <q>несообщающимися сосудами</q>: если данные отправлены
через один коммуникатор, ветвь-получатель
сможет принять их только через этот же
самый коммуникатор, но ни через какой-либо
другой. Как и группы,
коммуникаторы являются системными
объектами и доступны
программисту только через
<q>handle</q> (дескриптор).
Например, предопределенный
дескриптор MPI_COMM_WORLD. Один
процесс может быть членом нескольких
коммуникаторов.</p>

<p>Зачем вообще нужны разные группы, разные области связи и разные их описатели? По существу, они служат той же цели, что и
идентификаторы сообщений - помогают ветви-приемнику и ветви-получателю надежнее определять друг друга, а также
содержимое сообщения. Ветви внутри параллельного приложения могут объединяться в подколлективы для решения промежуточных задач – посредством
создания групп, и областей связи над группами. Пользуясь описателем этой области связи, ветви гарантированно ничего не примут извне подколлектива,
и ничего не отправят наружу.
Параллельно при этом они могут продолжать пользоваться любым другим имеющимся в их распоряжении коммуникатором для пересылок вне подколлектива, например, MPI_COMM_WORLD для обмена данными внутри всего приложения.
Коллективные функции создают дубликат от полученного аргументом коммуникатора, и передают данные через дубликат,
не опасаясь, что их сообщения будут случайно перепутаны с сообщениями функций <q>точка-точка</q>, распространяемыми через оригинальный коммуникатор.
Программист с этой же целью в разных кусках кода может передавать данные между ветвями через разные коммуникаторы, один из которых создан копированием другого.
</p>

<p>Коммуникаторы распределяются автоматически (функциями семейства <q>Создать новый комуникатор</q>),
и для них не существует джокеров (<q>принимай через какой угодно коммуникатор</q>) – вот еще два их существенных достоинства перед идентификаторами сообщений.
Идентификаторы (целые числа) распределяются пользователем вручную, и это служит источником двух частых ошибок вследствие
путаницы на приемной стороне. Сообщениям, имеющим разный смысл, вручную по ошибке назначается один и тот же идентификатор, а функция приема с джокером вообще
принимает все подряд, в том числе и те сообщения, которые должны быть приняты и обработаны в другом месте ветви.
</p>

<p><b>Важно помнить</b>, что <i><em>все</em></i> функции, создающие коммуникатор, являются <b>коллективными!</b> Именно это качество позволяет таким функциям возвращать в
разные ветви <i><em>один и тот же</em></i> описатель. Коллективность заключется
в следующем: одним из аргументов функции является коммуникатор, функцию должны вызывать <i><em>все</em></i> ветви-абоненты указываемого коммуникатора.
</p>

<p><em>Копирование.</em> Самый простой способ создания коммуникатора – скопировать <q>один-в-один</q> уже имеющийся:
</p>

<div class="lang-cpp"><![CDATA[
MPI_Comm tempComm;
MPI_Comm_dup(MPI_COMM_WORLD, &tempComm);
/* ... передаем данные через tempComm ... */
MPI_Comm_free(&tempComm);
]]></div>

<p>
Новая группа при этом не создается –
набор задач остается прежним. Новый
коммуникатор наследует все свойства
копируемого. 
</p>
<p><em>Расщепление.</em>
Соответствующая коммуникатору группа
расщепляется на непересекающиеся
подгруппы, для каждой из которых заводится
свой коммуникатор. 
</p>

<div class="lang-cpp"><![CDATA[
MPI_Comm_split(
    existingComm, /* существующий описатель, например MPI_COMM_WORLD */
    indexOfNewSubComm, /* номер подгруппы, куда надо поместить ветвь */
    rankInNewSubComm, /* желательный номер в новой подгруппе */
    &newSubComm); /* описатель области связи новой подгруппы */
]]></div>

<p>
Эта функция имеет одинаковый первый
параметр во всех ветвях, но разные второй
и третий – и в зависимости от них разные
ветви определяются в разные подгруппы;
возвращаемый в четвертом параметре
описатель будет принимать в разных
ветвях разные значения (всего столько
разных значений, сколько создано
подгрупп). Если indexOfNewSubComm равен
<b>MPI_UNDEFINED</b>, то в newSubComm вернется
MPI_COMM_NULL, т.е. ветвь не будет включена ни
в одну из созданных групп.</p>
<p><em>Создание
через группы.</em> В предыдущих двух случаях
коммуникатор создается от существующего
коммуникатора напрямую, без явного
создания группы: группа либо та же самая,
либо создается автоматически. Самый же
общий способ таков: Функцией <b>MPI_Comm_group</b>
определяется группа, на которую указывает
соответствующий коммуникатор. На базе
существующих групп функциями семейства
<b>MPI_Group_xxx</b> создаются новые группы с
нужным набором ветвей, для итоговой
группы функцией <b>MPI_Comm_create</b> создается
коммуникатор (не забудьте, что она должна
быть вызвана во ВСЕХ ветвях-абонентах
коммуникатора, передаваемого первым
параметром). Все описатели созданных
групп очищаются вызовами функции
<b>MPI_Group_free</b>. 
</p>

<p>Такой механизм позволяет, в частности, не только расщеплять группы подобно MPI_Comm_split, но и объединять их.
Всего в MPI определено 7 разных функций конструирования групп.</p>

<p>Типичное использование: извлечь дескриптор глобальной группы из MPI_COMM_WORLD с помощью MPI_Comm_group.
Сформировать новую группу как подмножество глобальной группы с помощью MPI_Group_incl.
Создать новый коммуникатор для новой группы с помощью MPI_Comm_create.
Определить новый идентификатор процесса в новом коммуникаторе с помощью MPI_Comm_rank.
Произвести коммуникации, используя любые процедуры MPI для передачи сообщений.
После окончания освобождаем новые коммуникатор и группу, используя MPI_Comm_free и MPI_Group_free.</p>

<p><b>MPI_Comm_group</b><b>.</b>
Возвращает дескриптор группы, связанной с данным коммуникатором.
</p>
<p>
<b>MPI_Comm_group(comm,*group)</b></p>
<p>
<b>MPI_COMM_GROUP(comm,group,ierr) </b>
</p>
<p><b>MPI_Group_rank</b><b>.</b>
Возвращает ранг процесса в группе
или MPI_UNDEFINED, если
процесс не входит в эту группу.

</p>
<p>
<b>MPI_Group_rank
(group,*rank) </b>
</p>
<p>
<b>MPI_GROUP_RANK
(group,rank,ierr) </b>
</p>
<p><b>MPI_Group_size</b><b>.</b>
Возвращает размер группы
– число процессов. 
</p>
<p>
<b>MPI_Group_size
(group,*size) </b>
</p>
<p>
<b>MPI_GROUP_SIZE
(group,size,ierr) </b>
</p>
<p><b>MPI_Group_excl.</b>
Создает группу путем переименования
существующей группы и включения только
неперечисленных членов. 
</p>
<p>
<b>MPI_Group_excl(group,n,*ranks,*newgroup) </b>
</p>
<p>
<b>MPI_GROUP_EXCL(group,n,ranks,newgroup,ierr) </b>
</p>
<p><b>MPI_Group_incl.</b>
Создает группу путем переименования
существующей группы и включения только
перечисленных членов. 
</p>
<p>
<b>MPI_Group_incl(group,n,*ranks,*newgroup) </b>
</p>
<p>
<b>MPI_GROUP_INCL(group,n,ranks,newgroup,ierr) </b>
</p>
<p><b>MPI_Group_intersection</b><b>.</b>
Создает группу из пересечения двух
групп. 
</p>
<p>
<b>MPI_Group_intersection
(group1,group2,*newgroup)</b></p>
<p>
<b>MPI_GROUP_INTERSECTION
(group1,group2,newgroup,ierr) </b>
</p>
<p><b>MPI_Group_union</b><b>.</b>
Создает группу из объединения двух
групп. 
</p>
<p>
<b>MPI_Group_union
(group1,group2,*newgroup) </b>
</p>
<p>
<b>MPI_GROUP_UNION
(group1,group2,newgroup,ierr) </b>
</p>
<p><b>MPI_Group_difference</b><b>.</b>
Создает группу из разности двух
групп. 
</p>
<p>
<b>MPI_Group_difference(group1,group2,*newgroup) </b>
</p>
<p>
<b>MPI_GROUP_DIFFERENCE(group1,group2,newgroup,ierr) </b>
</p>
<p><b>MPI_Group_compare.</b>
Сравнивает две группы и возвращает
целое число: MPI_IDENT – если порядок и
состав членов в обеих группах одинаков,
MPI_SIMILAR – если только состав одинаков,
и MPI_UNEQUAL – в остальных случаях. 
</p>
<p>
<b>MPI_Group_compare(group1,group2,*result) </b>
</p>
<p>
<b>MPI_GROUP_COMPARE(group1,group2,result,ierr) </b>
</p>
<p><b>MPI_Group_free</b><b>.</b>
Освобождает группу (системный
объект). 
</p>
<p>
<b>MPI_Group_free
(group) </b>
</p>
<p>
<b>MPI_GROUP_FREE
(group,ierr) </b>
</p>
<p><b>MPI_Comm_create</b><b>.
</b>Создает новый коммуникатор из старого
коммуникатора и новой группы.

</p>
<p>
<b>MPI_Comm_create(comm,group,*newcomm) </b>
</p>
<p>
<b>MPI_COMM_CREATE(comm,group,newcomm,ierr) </b>
</p>
<p><b>MPI_Comm_compare.
</b>Сравнивает два коммуникатора<b> </b>и
возвращает целое число: MPI_IDENT – если
контекст и группа одни и те же, MPI_CONGRUENT
– если разный контекст, но одинаковая
группа, MPI_SIMILAR – если разный контекст,
но сходная группа, а иначе – MPI_UNEQUAL. 
</p>
<p>
<b>MPI_Comm_compare(comm1,comm2,*result) </b>
</p>
<p>
<b>MPI_COMM_COMPARE(comm1,comm2,result,ierr) </b>
</p>
<p><b>MPI_Comm_free</b><b>:
</b>Освобождает коммуникатор.

</p>
<p>
<b>MPI_Comm_free
(*comm)</b></p>
<p>
<b>MPI_COMM_FREE
(comm,ierr) </b>
</p>
<p>Пример:
Создание двух различных групп процессов
для раздельного коллективного
обмена сообщениями. Требуется
также создание новых коммуникаторов.
</p>

<div class="lang-cpp"><![CDATA[
#include <mpi.h>
#include <stdio.h>
#define NPROCS 8
int main(int argc, char **argv) {
    int rank, new_rank, sendbuf, recvbuf, numtasks,
        ranks1[4]={0,1,2,3}, ranks2[4]={4,5,6,7};
    MPI_Group orig_group, new_group;
    MPI_Comm new_comm;
    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    if(numtasks != NPROCS) {
        printf("Must specify MP_PROCS= %d. Terminating.\n", NPROCS);
        MPI_Finalize();
        exit(0);
   }
   sendbuf = rank;
   /* Extract the original group handle */
   MPI_Comm_group(MPI_COMM_WORLD, &orig_group);
   /* Divide tasks into two distinct groups based upon rank */
   if(rank < NPROCS/2) {
     MPI_Group_incl(orig_group, NPROCS/2, ranks1, &new_group);
   } else {
     MPI_Group_incl(orig_group, NPROCS/2, ranks2, &new_group);
   }
   /* Create new communicator and then perform collective communications */
   MPI_Comm_create(MPI_COMM_WORLD, new_group, &new_comm);
   MPI_Allreduce(&sendbuf, &recvbuf, 1, MPI_INT, MPI_SUM, new_comm);
   MPI_Group_rank(new_group, &new_rank);
   printf("rank=%d newrank= %d recvbuf= %d\n",rank,new_rank,recvbuf);
   MPI_Finalize();
   return 0;
}
]]></div>

<p>Пример программного вывода:</p>

<p><b>rank=7 newrank= 3 recvbuf= 22</b></p>
<p><b>rank=0 newrank= 0 recvbuf= 6</b></p>
<p><b>rank=1 newrank= 1 recvbuf= 6</b></p>
<p><b>rank=2 newrank= 2 recvbuf= 6</b></p>
<p><b>rank=6 newrank= 2 recvbuf= 22</b></p>
<p><b>rank=3 newrank= 3 recvbuf= 6</b></p>
<p><b>rank=4 newrank= 0 recvbuf= 22</b></p>
<p><b>rank=5 newrank= 1 recvbuf= 22</b></p>

<p>Может ли задача обратиться к области связи,
абонентом которой не является? <br/>Нет.
Описатель области связи передается в
задачу функциями MPI, которые одновременно
делают эту задачу абонентом описываемой
области. Таков единственный существующий
способ получить описатель. Попытки
<q>пиратскими</q> средствами обойти
это препятствие (например, получить
описатель, посредством MPI_Send/MPI_Recv
переслать его в другую задачу, не
являющуюся его абонентом, и там им
воспользоваться) не приветствуются, и
исход их, скорее всего, будет определяться
деталями реализации. 
</p>
<p><i><em><b>Полезная нагрузка коммуникатора: атрибуты.</b></em></i>
Помимо характеристик области связи,
тело коммуникатора содержит в себе
некие дополнительные данные (атрибуты).
Механизм хранения атрибутов называется <b><q>caching</q></b>. Атрибуты могут быть
системные и пользовательские; в системных,
в частности, хранятся: адрес
функции-обработчика ошибок; описание
пользовательской топологии; максимально
допустимый идентификатор для сообщений.
</p>
<p>Атрибуты
идентифицируются целыми числами, которые
MPI назначает автоматически. Некоторые
константы для описания системных
атрибутов: MPI_TAG_UB, MPI_HOST, MPI_IO,
MPI_WTIME_IS_GLOBAL. К этим атрибутам программист
обращается редко, и менять их не может;
а для таких часто используемых атрибутов,
как обработчик ошибок или описание
топологии, существуют персональные
наборы функций, например, <b>MPI_Errhandler_xxx</b>.
</p>
<p><dfn>Атрибуты</dfn> – удобное место хранения совместно
используемой информации; помещенная в
атрибут одной из ветвей, такая информация
становится доступной всем использующим
коммуникатор ветвям <i><em>без</em></i> пересылки
сообщений (вернее, на MPP-машине, к примеру,
сообщения будут, но на системном уровне,
т.е. скрытые от глаз программиста). 
</p>
<p>Пользовательские
атрибуты создаются и уничтожаются
функциями <b>MPI_Keyval_create</b> и <b>MPI_Keyval_free</b>;
модифицируются функциями <b>MPI_Attr_put</b>,
<b>MPI_Attr_get</b> и <b>MPI_Attr_delete</b>. При создании
коммуникатора на базе существующего
атрибуты из последнего тем или иным
образом копируются или нет в зависимости
от функции копирования типа
<b>MPI_Copy_function</b>, адрес которой является
параметром функции создания атрибута.
То же и для удаления атрибутов при
уничтожении коммуникатора: задается
пользовательской функцией типа
<b>MPI_Delete_function</b>, указываемой при создании
атрибута. 
</p>

<p><i><em><b>Корректное удаление отслуживших
описателей.</b></em></i> Здесь имеются в виду
ВСЕ типы системных данных, для которых
предусмотрена функция MPI_Xxx_free (и константа
MPI_XXX_NULL). В MPI-I их 7 штук: коммуникаторы;
группы; типы данных; распределенные
операции; квитанции (request's); атрибуты
коммуникаторов; обработчики ошибок
(errhandler's). Дальше все описывается на
примере коммуникаторов и групп, но
изложенная схема является общей для
всех типов ресурсов. 
</p>
<p>Не
играет роли, в каком порядке уничтожать
взаимосвязанные описатели. Главное -
не забыть вызвать функцию удаления
ресурса MPI_Xxx_free вовсе. Соответствующий
ресурс не будет удален немедленно, он
прекратит существование, только если
будут выполнены два условия: программе
пользователя никогда не предоставлялись
ссылки на ресурс, или все пользовательские
ссылки очищены вызовами MPI_Xxx_free; ресурс
перестает использоваться другими
ресурсами MPI, то есть удаляются все
системные ссылки. 
</p>
<p>Взаимосвязанными
описателями являются описатели
коммуникатора и группы,
(коммуникатор ссылается на группу);
или описатели типов, если один создан
на базе другого (порожденный тип ссылается
на исходный). Пример: 
</p>

<div class="lang-cpp"><![CDATA[
MPI_Comm subComm;
MPI_Group subGroup;
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_split(MPI_COMM_WORLD, rank / 3, rank % 3, &subComm);
]]></div>

<p>
 /*
Теперь создан коммуникатор subComm, и
автоматически создана группа, на которую
распространяется его область действия.
На коммуникатор заведена ссылка из
программы - subComm. На группу заведена
системная ссылка из коммуникатора.  */</p>

<p>MPI_Comm_group (subComm, &amp;subGroup);</p>
<p>
 /*
Теперь на группу имеется две ссылки -
системная из коммуникатора, и
пользовательская subGroup.  */</p>

<p>MPI_Group_free (&amp;subGroup);</p>
<p>
 /*
Пользовательская ссылка на группу
уничтожена, subGroup сброшен в MPI_GROUP_NULL.
Собственно описание группы из системных
данных не удалено, так как на него еще
ссылается коммуникатор.  */</p>

<p>MPI_Comm_free(&amp;subComm);</p>

<p>
 /*
Удалена пользовательская ссылка на
коммуникатор, subComm сброшен в MPI_COMM_NULL.
Так как других ссылок на коммуникатор
нет, его описание удаляется из системных
данных. Вместе с коммуникатором удалена
системная ссылка на группу. Так как
других ссылок на группу нет, ее описание
удаляется из системных данных. */</p>

<p>Для MPI не играет роли, в каком порядке
будут вызваны завершающие вызовы
MPI_Xxx_free, это дело программы. И не пытайтесь
уничтожать константные описатели вроде
MPI_COMM_WORLD или MPI_CHAR: их создание и уничтожение
- дело самого MPI. 
</p>

<h1 id="__RefHeading___Toc5075837">Виртуальные пользовательские топологии</h1>

<p>Внутри
группы задачи пронумерованы линейно
от 0 до (размер группы-1). Однако через
коммуникатор можно <i><em>дополнительно</em></i>
навязать для них еще одну систему
нумерации. Таких дополнительных систем
в MPI две: декартова n-мерная решетка (с
цикличностью и без оной), а также
биориентированный граф. Предоставляются
функции для создания нумераций
(MPI_Topo_test, MPI_Cart_xxx, MPI_Graph_xxx) и для преобразования
номеров из одной системы в другую. Этот
механизм не должен восприниматься как
предоставляющий возможность подгонки
связей между ветвями под аппаратную
топологию для повышения быстродействия
– он всего лишь автоматизирует перерасчет
адресов, которым должны заниматься
ветви, скажем, при вычислении матриц:
через коммуникатор задается картезианская
система координат, где координаты ветви
совпадают с координатами вычисляемой
ею подматрицы. 
</p>

<p><b>MPI_Cart_coords.</b> Возвращает координаты процесса в декартовой топологии.</p>
<p>
<b>MPI_Cart_coords(comm,rank,maxdims,*coords[]) </b>
</p>
<p>
<b>MPI_CART_COORDS(comm,rank,maxdims,coords(),ierr) </b>
</p>
<p><b>MPI_Cart_create</b><b>.</b>
Создание нового коммуникатора с
декартовой топологией.</p>
<p>
<b>MPI_Cart_create
(comm_old,ndims,*dims[],*periods,reorder,*comm_cart) </b>
</p>
<p>
<b>MPI_CART_CREATE</b><b>
</b><b>(comm_old,ndims,dims(),periods,reorder,comm_cart,ierr)
</b>
</p>
<p><b>MPI_Cart_get</b><b>.</b>
Получить число размерностей,
координаты и периодичность
для данного процесса в декартовой
топологии. 
</p>
<p>
<b>MPI_Cart_get(comm,maxdims,*dims,*periods,*coords[]) </b>
</p>
<p>
<b>MPI_CART_GET(comm,maxdims,dims,periods,coords(),ierr) </b>
</p>
<p><b>MPI_Cart_map</b><b>.</b>
Задать для процессов декартову
топологию. 
</p>
<p>
<b>MPI_Cart_map(comm_old,ndims,*dims[],*periods[],*newrank) </b>
</p>
<p>
<b>MPI_CART_MAP(comm_old,ndims,dims(),periods(),newrank,ierr) </b>
</p>
<p><b>MPI_Cart_rank</b><b>.</b>
Определить ID процесса
в коммуникаторе по его декартовым
координатам. 
</p>
<p>
<b>MPI_Cart_rank
(comm,*coords[],*rank) </b>
</p>
<p>
<b>MPI_CART_RANK
(comm,coords(),rank,ierr) </b>
</p>
<p><b>MPI_Cart_shift</b><b>.</b>
Возвращает ID источника
и приемника для циклического сдвига в
декартовой топологии. Текущий
процесс задает направление и длину
сдвига. 
</p>
<p>
<b>MPI_Cart_shift
(comm,direction,displ,*source,*dest) </b>
</p>
<p>
<b>MPI_CART_SHIFT(comm,direction,displ,source,dest,ierr) </b>
</p>
<p><b>MPI_Cart_sub</b><b>.</b>
Разделяет коммуникатор на подгруппы,
из которых формируются
декартовы решетки меньшего размера.

</p>
<p>
<b>MPI_Cart_sub(comm,*remain_dims[],*comm_new) </b>
</p>
<p>
<b>MPI_CART_SUB(comm,remain_dims(),comm_new,ierr) </b>
</p>
<p><b>MPI_Cartdim_get</b><b>.</b>
Получить число размерностей в
декартовой топологии коммуникатора.

</p>
<p>
<b>MPI_Cartdim_get
(comm,*ndims) </b>
</p>
<p>
<b>MPI_CARTDIM_GET
(comm,ndims,ierr) </b>
</p>
<p><b>MPI_Dims_create</b><b>.</b>
Создает разделение
процессов в декартовской
сетке.</p>
<p>
<b>MPI_Dims_create
(nnodes,ndims,*dims[]) </b>
</p>
<p>
<b>MPI_DIMS_CREATE(nnodes,ndims,dims(),ierr) </b>
</p>
<p><b>MPI_Graph_create</b><b>.</b>
Создание нового коммуникатора с
топологией графа. 
</p>
<p>
<b>MPI_Graph_create(comm_old,nnodes,*index[],*edges[],reorder,*comm_graph)</b></p>
<p>
<b>MPI_GRAPH_CREATE(comm_old,nnodes,index(),edges(),
reorder,comm_graph,ierr)</b></p>
<p><b>MPI_Graph_get</b><b>.</b>
Получить информацию о топологии
графа для данного коммуникатора.

</p>
<p>
<b>MPI_Graph_get(comm,maxindex,maxedges,*index[],*edges[]) </b>
</p>
<p>
<b>MPI_GRAPH_GET(comm,maxindex,maxedges,index(),edges(),ierr) </b>
</p>
<p><b>MPI_Graph_map</b><b>.</b>
Задать для проыессов топологию
графа. 
</p>
<p>
<b>MPI_Graph_map(comm_old,nnodes,*index[],*edges[],*newrank)</b></p>
<p>
<b>MPI_GRAPH_MAP(comm_old,nnodes,index(),edges(),newrank,ierr) </b>
</p>
<p><b>MPI_Graph_neighbors</b><b>.</b>
Получить соседей данного процесса
в графовой топологии. 
</p>
<p>
<b>MPI_Graph_neighbors(comm,rank,maxneighbors,*neighbors[])</b></p>
<p>
<b>MPI_GRAPH_NEIGHBORS(comm,rank,maxneighbors,neighbors(),ierr) </b>
</p>
<p><b>MPI_Graphdims_get</b><b>.</b>
Получить информацию о топологии
графа (число узлов и число
ребер) для данного
коммуникатора. 
</p>
<p>
<b>MPI_Graphdims_get
(comm,*nnodes,*nedges)</b></p>
<p>
<b>MPI_GRAPHDIMS_GET
(comm,nnodes,nedges,ierr) </b>
</p>
<p><b>MPI_Topo_test</b><b>.</b>
Определить тип топологии коммуникатора.</p>
<p>
<b>MPI_Topo_test(comm,*top_type)</b></p>
<p>
<b>MPI_TOPO_TEST(comm,top_type,ierr)</b></p>


<table width="50%">
  <tr>
   <td>0<br/>(0,0)</td>
   <td>1<br/>(0,1)</td>
   <td>2<br/>(0,2)</td>
   <td>3<br/>(0,3)</td>
  </tr>
  <tr>
   <td>4<br/>(1,0)</td>
   <td>5<br/>(1,1)</td>
   <td>6<br/>(1,2)</td>
   <td>7<br/>(1,3)</td>
  </tr>
  <tr>
   <td>8<br/>(2,0)</td>
   <td>9<br/>(2,1)</td>
   <td>10<br/>(2,2)</td>
   <td>11<br/>(2,3)</td>
  </tr>
  <tr>
   <td>12<br/>(3,0)</td>
   <td>13<br/>(3,1)</td>
   <td>14<br/>(3,2)</td>
   <td>15<br/>(3,3)</td>
  </tr>
 </table>

<p>Пример: Работа с 4x4 декартовой топологией из 16 процессов.</p>

<div class="lang-cpp"><![CDATA[
    #include <mpi.h>
    #include <stdio.h>
    #define SIZE 16
    #define UP    0
    #define DOWN  1
    #define LEFT  2
    #define RIGHT 3
    int main(int argc, char **argv) {
    int  numtasks, rank, source, dest, outbuf, i, tag=1, 
    inbuf[4]={MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,MPI_PROC_NULL,},
       nbrs[4], dims[2]={4,4}, periods[2]={0,0},
    reorder=0, coords[2];
    MPI_Request reqs[8];
    MPI_Status stats[8];
    MPI_Comm cartcomm;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
    if (numtasks == SIZE) {
      MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, reorder, &cartcomm);
      MPI_Comm_rank(cartcomm, &rank);
      MPI_Cart_coords(cartcomm, rank, 2, coords);
      MPI_Cart_shift(cartcomm, 0, 1, &nbrs[UP], &nbrs[DOWN]);
      MPI_Cart_shift(cartcomm, 1, 1, &nbrs[LEFT], &nbrs[RIGHT]);
      outbuf = rank;
      for(i=0; i < 4; i++) {
         dest = nbrs[i];
         source = nbrs[i];
         MPI_Isend(&outbuf, 1, MPI_INT, dest, tag, MPI_COMM_WORLD, &reqs[i]);
         MPI_Irecv(&inbuf[i], 1, MPI_INT, source, tag, MPI_COMM_WORLD, &reqs[i+4]);
         }
      MPI_Waitall(8, reqs, stats);
      printf("rank=%d coords= %d %d  neighbors(u,d,l,r)= %d %d %d %d\n",
            rank,coords[0],coords[1],nbrs[UP],nbrs[DOWN],nbrs[LEFT], nbrs[RIGHT]);
      printf("rank=%d inbuf(u,d,l,r)= %d %d %d %d\n",
    rank,inbuf[UP],inbuf[DOWN],inbuf[LEFT],inbuf[RIGHT]);
      }
    else
      printf("Must specify %d processors. Terminating.\n", SIZE);
    MPI_Finalize();
}
]]></div>

<p>Пример программного вывода: (частично)
</p>

<p>rank=0 coords= 0 0  neighbors(u,d,l,r)= -3 4 -3 1</p>
<p>rank=0                inbuf(u,d,l,r)= -3 4 -3 1</p>
<p>rank=1 coords= 0 1  neighbors(u,d,l,r)= -3 5 0 2</p>
<p>rank=1                inbuf(u,d,l,r)= -3 5 0 2</p>
<p>rank=2 coords= 0 2  neighbors(u,d,l,r)= -3 6 1 3</p>
<p>rank=2                inbuf(u,d,l,r)= -3 6 1 3</p>

<p>
       .
. . . .</p>
<p><br/>
</p>
<p>rank=14 coords= 3 2  neighbors(u,d,l,r)= 10 -3 13 15</p>
<p>rank=14                inbuf(u,d,l,r)= 10 -3 13 15</p>
<p>rank=15 coords= 3 3  neighbors(u,d,l,r)= 11 -3 14 -3</p>
<p>rank=15                inbuf(u,d,l,r)= 11 -3 14 -3</p>

<h1 id="__RefHeading___Toc5075838">Второстепенные детали</h1>

<p><em>Обработчики ошибок</em>. По умолчанию,
если при выполнении функции MPI обнаружена
ошибка, выполнение всех ветвей приложения
завершается. Это сделано в расчете на
неряшливого программиста, не привыкшего
проверять коды завершения (malloc,
open, write, и
т. д.), и пытающегося распространить
такой стиль на MPI. При аварийном завершении
по такому сценарию на консоль выдается
очень скудная информация: в лучшем
случае, там будет название функции MPI и
название ошибки. Обработчик ошибок
является принадлежностью коммуникатора,
для управления обработчиками служат
функции семейства <b>MPI_Errhandler_xxx</b>.</p>
<p><em>Многопоточность</em>. Сам MPI неявно
использует многопоточность очень
широко, и не мешает программисту делать
то же самое. Однако: разные задачи имеют
с точки зрения MPI ОБЯЗАТЕЛЬНО разные
номера, а разные потоки (threads) внутри
одной задачи для него ничем не отличаются.
Программист сам идентификаторами
сообщений и коммуникаторами должен
устанавливать такую дисциплину для
потоков, чтобы один поток не стал,
допустим, вызывая MPI_Recv, джокером
перехватывать сообщения, которые должен
принимать и обрабатывать другой поток
той же задачи. Другим источником ошибок
может быть использование разными
потоками коллективных функций над одним
и тем же коммуникатором: используйте
MPI_Comm_dup! 
</p>
<p>
<em>Работа с файлами</em>. В MPI-2 средства
перенаправления работы с файлами
появились, в MPI-1 их нет. Все вызовы функций
напрямую передаются операционной
системе (Unix/Parix/NFS/...) на той машине, или
на том процессорном узле MPP-компьютера,
где находится вызывающая ветвь.
Теоретически возможность подключения
средств расширенного управления
вводом/выводом в MPI-1 есть – каждый
коммуникатор хранит атрибут с числовым
кодом MPI_IO – это номер ветви, в которую
перенаправляется ввод/вывод от всех
остальных ветвей в коммуникаторе; сам
MPI ничего с ним не делает и никак не
использует. 
</p>
<p>Работа
с консолью также отдается на откуп
системе; это может приводить к перемешиванию
вывода нескольких задач, поэтому
рекомендуется весь вывод на экран
производить из какой-то одной задачи
(обычно нулевой). 
</p>
<p><dfn>MPI 2.0</dfn>. В марте 1995 MPI Форум начал
обсуждение расширений к
стандарту MPI. Эти расширения
стали теперь частью стандарта
MPI-2. Ключевые особенности
новых функциональных возможностей: 
</p>
<p>1)
Динамические процессы: расширение, 
удаляющее статическую модель процесса
MPI. Обеспечивает процедуры для создания
новых процессов. 
</p>
<p>2)
Односторонние коммуникации: процедуры
для однонаправленных коммуникаций.
Включают в себя операции с общей памятью
(put/get), и
удаленные операции. 
</p>
<p>3) Расширенные коллективные действия:
не-блокируемые коллективные операции
и приложение коллективных операций к
интракоммуникаторам 
</p>
<p>4) Внешние интерфейсы: определяет стандартные процедуры для профилирования и отладки MPI программ. 
</p>
<p>5) Поддержка MPI в языках: C++ и ФОРТРАН -90. 
</p>
<p>6) Параллельный ввод/вывод.
</p>


</body>
</html>